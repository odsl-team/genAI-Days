{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import corner\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Apple Silicon or CUDA is available, use the GPU.\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Normalizing Flows Tutorial\n",
    "In this tutorial, you will work with simulated gravitational wave data and learn the posterior distribution $p(\\theta|d)$ over the two black hole masses $\\theta = [m_1, m_2]$. The data $d$ corresponds to the simulated strain $h_+$ (real and imaginary part), whitened with realistic detector noise of the LIGO detector: \n",
    "$$d = h_+(\\theta) + n \\hspace{0.5cm} \\mathrm{with}~ n \\sim p_{S_n}(n)$$\n",
    "where the noise is taken to be a stationary Gaussian with some power spectral density $S_n$. More information about the data simulation details can be found in the notebook `01_data_generation.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation\n",
    "Before we can start with training, we need to generate a training data set. You can find the code for this in the notebook `01_data_generation.ipynb`. You can simply run all cells in the notebook (you might need to adjust the path when saving the data).\n",
    "If you are interested in gravitational wave data and how the simulation works, you can go through all explanations and visualizations provided in the data generation notebook. However, this is not necessary for the normalizing flow tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing for training\n",
    "In this notebook, we will load and preprocess the data generated with the notebook `data_generation.ipynb` and saved as `data/training_dataset.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the absolute path of the directory where the notebook is located\n",
    "print(\"Current working directory\", os.getcwd())\n",
    "data_folder = os.path.join(os.getcwd(), 'genAI-Days/01_normalizing_flows/data')\n",
    "file_name = os.path.join(data_folder, 'dataset.pkl')\n",
    "\n",
    "if not os.path.isfile(file_name):\n",
    "    raise ValueError(f\"File {file_name} does not exist, correct path or generate data set.\")\n",
    "\n",
    "with open(file_name, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "print('Sucessfully loaded dataset with', len(data['hp']), 'waveforms.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Inspect the contents of `data`.\n",
    "\n",
    "Here is some general information about gravitational wave data:\n",
    "A gravitational wave consists of two polarizations $h_+(\\theta)$ and $h_\\times(\\theta)$ defined in frequency domain which are simulated based on some prior parameter values $\\theta$.\n",
    "In addition to the polarizations, we need information about the frequency range on which the signal is defined.\n",
    "\n",
    "Can you extract the relevant properties from `data`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polarization_hp, polarization_hc = np.array(...), np.array(...)\n",
    "# parameters = np.array(..., dtype=np.float32)\n",
    "# f_min, f_max, T = ..., ..., ...\n",
    "# delta_f = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this information, we can preprocess the waveforms:\n",
    "- It might be the case the 0 values are saved in the polarization below $f_\\mathrm{min}$. Therefore, we truncate the polarizations below $f_\\mathrm{min}$.\n",
    "- To simplify data handling and training, we re-package real and imaginary part of the polarization and only use $h_+$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_cut = int(f_min / delta_f)\n",
    "waveforms = np.hstack((polarization_hp.real[:, lower_cut:], polarization_hp.imag[:, lower_cut:])).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: To make training easy for the normalizing flow, we need to standardize the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters_standardized = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we wrap waveforms in a pytorch `Dataset` which returns the parameters and the signal \n",
    "$$d = h_+(\\theta) + n, \\hspace{0.5cm} \\text{with } n \\sim \\mathcal{N}(0,1)$$\n",
    "\n",
    "Task 3: Complete the code for the dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveformDataset(Dataset):\n",
    "    def __init__(self, parameters, waveforms):\n",
    "        self.parameters = parameters\n",
    "        self.waveforms = waveforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.parameters)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # params = ...\n",
    "        # signal = ...\n",
    "        \n",
    "        # Add unit normal noise to the signal\n",
    "        # noise =  ... .astype(np.float32)\n",
    "        # data = ...\n",
    "\n",
    "        if device == \"mps\":\n",
    "            return torch.tensor(data, dtype=torch.float32, device=device), torch.tensor(params,  dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            return torch.tensor(data, device=device), torch.tensor(params, device=device)\n",
    "    \n",
    "waveform_dataset = WaveformDataset(parameters_standardized, waveforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Visualize the data $d$ from an arbitrary sample of the `waveform_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first sample (Careful to put data back onto CPU)\n",
    "# Plot signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test dataset\n",
    "Task 5: Determine the number of training and test samples from a `train_fraction` and use the function `random_split()` to separate the data into a training and test dataset. Afterwards define the training and test dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.8\n",
    "# num_samples = ...\n",
    "# num_train = ...\n",
    "# num_test = ...\n",
    "train_dataset, test_dataset = random_split(waveform_dataset, [num_train, num_test])\n",
    "\n",
    "# The DataLoader is used in training\n",
    "# train_dataloader = DataLoader(...)\n",
    "# test_dataloader = DataLoader(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Normalizing Flow\n",
    "We use the `glasflow` library. It is a wrapper for [`nflows`](https://github.com/bayesiains/nflows), which implements different flow types (e.g. autoregressive flows and coupling flows) and transforms (e.g. rational-quadratic splines).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from glasflow.nflows import transforms, distributions, flows\n",
    "import glasflow.nflows as nflows\n",
    "import glasflow.nflows.nn.nets as nflows_nets\n",
    "\n",
    "device = 'cpu'  # For some reason, \"mps\" does not work here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Code is adapted from examples in the [neural spline flow repository](https://github.com/bayesiains/nsf).\n",
    "\n",
    "First, we define a function that creates the basic MAF or RQS-coupling transform.\n",
    "\n",
    "Task 6: Understand the arguments of `create_base_transform`. Set default arguments for `hidden_dim`, `num_transform_blocks`, `batch_norm`, and `base_transform_type`. Define which activation fucntion to use and how many bins the rational-quadratic spline transformation should utilize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_transform(\n",
    "    i: int,\n",
    "    param_dim: int,\n",
    "    context_dim: int = None,\n",
    "    hidden_dim: int = ..., # TODO\n",
    "    num_transform_blocks: int = ..., # TODO\n",
    "    batch_norm: bool = ..., # TODO\n",
    "    base_transform_type: str = ..., # TODO\n",
    "):\n",
    "    \n",
    "    activation_fn = ... # TODO\n",
    "\n",
    "    if base_transform_type == \"maf\":\n",
    "        return transforms.MaskedAffineAutoregressiveTransform(\n",
    "            param_dim,\n",
    "            hidden_features=hidden_dim,\n",
    "            context_features=context_dim,\n",
    "            num_blocks=num_transform_blocks,\n",
    "            activation=activation_fn,\n",
    "            use_batch_norm=batch_norm,\n",
    "        )\n",
    "    elif base_transform_type == \"rq-coupling\":\n",
    "        if param_dim == 1:\n",
    "            mask = torch.tensor([1], dtype=torch.uint8)\n",
    "        else:\n",
    "            mask = nflows.utils.create_alternating_binary_mask(\n",
    "                param_dim, even=(i % 2 == 0)\n",
    "            )\n",
    "        return transforms.PiecewiseRationalQuadraticCouplingTransform(\n",
    "            mask=mask,\n",
    "            transform_net_create_fn=(\n",
    "                lambda in_features, out_features: nflows_nets.ResidualNet(\n",
    "                    in_features=in_features,\n",
    "                    out_features=out_features,\n",
    "                    hidden_features=hidden_dim,\n",
    "                    context_features=context_dim,\n",
    "                    num_blocks=num_transform_blocks,\n",
    "                    activation=activation_fn,\n",
    "                    use_batch_norm=batch_norm,\n",
    "                )\n",
    "            ),\n",
    "            num_bins= ..., # TODO\n",
    "            tails=\"linear\",\n",
    "            tail_bound=1.0,\n",
    "            apply_unconditional_transform=False,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define a transform the randomly permutes the dimensions to ensure that all dimensions interact sufficiently with each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_linear_transform(param_dim: int):\n",
    "    return transforms.CompositeTransform(\n",
    "        [\n",
    "            transforms.RandomPermutation(features=param_dim),\n",
    "            transforms.LULinear(param_dim, identity_init=True),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can define the `create_transform` function which composes a list of `num_flow_steps` transforms where each transform consists of a linear transform (to permute dimensions) and a base transform (MAF or RQ-coupling).\n",
    "We start and finish the transforms with a linear transform that ensures that the flow outputs samples that have the correct dimensionality (`param_dim`).\n",
    "\n",
    "Task 7: How do we have to pass the arguments of `create_transform` on to `create_linear_transform` and `create_base_transform`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transform(\n",
    "    num_flow_steps: int, \n",
    "    param_dim: int, \n",
    "    context_dim: int, \n",
    "    base_transform_kwargs: dict\n",
    "):\n",
    "    return transforms.CompositeTransform(\n",
    "        [\n",
    "            transforms.CompositeTransform(\n",
    "                [\n",
    "                    create_linear_transform(...), # TODO\n",
    "                    create_base_transform(\n",
    "                        ..., **base_transform_kwargs # TODO\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "            for i in range(num_flow_steps)\n",
    "        ]\n",
    "        + [create_linear_transform(...)] # TODO\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire flow model consists of a standard normal base distribution, followed by the flow transform.\n",
    "\n",
    "Task 8: Fill in the arguments of `create_transform` and the base distribution such that it runs for the gravitational wave data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = create_transform(num_flow_steps = ..., # TODO\n",
    "                             param_dim = ..., # TODO\n",
    "                             context_dim = ..., # TODO\n",
    "                             base_transform_kwargs = {\"base_transform_type\": ...}) # TODO\n",
    "\n",
    "base_distribution = distributions.StandardNormal(shape=[...]) # TODO\n",
    "\n",
    "model = flows.Flow(transform, base_distribution)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Task 9: Define the loss function for training the normalizing flow based on `model.log_probs(...)` in the training and test loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test loops\n",
    "\n",
    "def train_loop(dataloader, model, optimizer):\n",
    "\n",
    "    model.train()\n",
    " \n",
    "    size = len(dataloader.dataset)\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute negative log probability loss  \n",
    "        log_probs = ... # TODO\n",
    "\n",
    "        loss = ... # TODO\n",
    "        \n",
    "        train_loss += loss.detach().sum()\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"Loss: {loss:>7f}  [{current:>5d}/{size:>5d} samples]\")\n",
    "            \n",
    "    average_loss = train_loss.item() / size\n",
    "    print('Average loss: {:.4f}'.format(average_loss))\n",
    "    return average_loss\n",
    "\n",
    "        \n",
    "def test_loop(dataloader, model):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            log_probs  = ... # TODO\n",
    "            loss = ... # TODO\n",
    "            test_loss += loss.sum()\n",
    "\n",
    "    test_loss = test_loss.item() / size\n",
    "    print(f\"Test loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 10: Define the parameters for the optimizer and write a loop that calls the functions `train_loop` and `test_loop` and stores the loss values in `train_history` and `test_history`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=..., weight_decay=...) # TODO\n",
    "\n",
    "epochs = ... # TODO\n",
    "train_history = []\n",
    "test_history = []\n",
    "for t in range(epochs):\n",
    "    ... # TODO\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 11: Plot the training and test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, len(train_history) + 1)\n",
    "... # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize posteriors\n",
    "Finally, we can evaluate the posteriors of the trained normalizing flow based on the test dataset.\n",
    "For this, we need to sample from the posterior, undo the standardization and visualize the samples in a corner plot.\n",
    "\n",
    "Tasl 12: Complete the lines that reverse the standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_posteriors = 3\n",
    "num_eval_samples = 10_000\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for n in range(num_posteriors):\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        test_x, test_y = test_dataset[n]\n",
    "       \n",
    "        # Sample the posterior\n",
    "        test_x = test_x.expand(num_eval_samples, *test_x.shape)\n",
    "        pred_samples = model.sample(1, test_x).squeeze(1).cpu().numpy()\n",
    "    \n",
    "        # Undo the standardization\n",
    "        pred_samples = ... # TODO\n",
    "        truth = ... # TODO\n",
    "    \n",
    "        # Plot\n",
    "        corner.corner(pred_samples, truths=truth, labels=['$m_1$', '$m_2$'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Tasks:\n",
    "- Visualize the distribution of masses in the training data set. What do you observe? Why is this parametrization not ideal?\n",
    "- In the gravitational waves community, the two masses $m_1, m_2$ of the black holes are usually not estimated directly, but they are re-parametrized in terms of the chirp mass $\\mathcal{M} = \\frac{(m_1 \\cdot m_2)^{3/5}}{(m_1 + m_2)^{1/5}}$ and mass ratio $q = \\frac{m_2}{m_1}$. Re-define the input parameters, visualize the distribution of the training data set and re-train the flow. Does this improve training?\n",
    "- The function `create_base_transform` defines a masked autoregressive flow (MAF) and a rational-quadratic spline (RQS) flow. Which flow performs better?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
