{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pVwytbD4t54"
   },
   "source": [
    "# Diffusion Model\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/odsl-team/genAI-Days/blob/main/03_diffusion_models/diffusion_model.ipynb)\n",
    "\n",
    "**Goal:** We just learned about Diffusion models, both the stochastic differential equation continuous formulation and the way to solve them in discrete space with Deep Denoising Probabilistic Models.\n",
    "\n",
    "This afternoon we're going to be working with the _same_ binary black hole merger dataset from Annalena, but now... doing neural posterior estimation with a diffusion model\n",
    "\n",
    "**Resources:**\n",
    "- Diffusion model tutorial from Sofia Palacios + Theo Heimel [https://colab.research.google.com/drive/1Hlb3qpNvvfi2oBJ99pgoXrB6AF86fFZf?usp=sharing]\n",
    "\n",
    "### Table of Contents\n",
    "1. [Preprocessing](#Preprocessing)\n",
    "2. [Defining the model](#model)\n",
    "3. [Train the model](#train)\n",
    "4. [Look at posteriors](#posteriors)\n",
    "\n",
    "Nicole Hartman\n",
    "\n",
    "ODSL GenAI Days\n",
    "\n",
    "24 Sept 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3422,
     "status": "ok",
     "timestamp": 1727147251803,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "_YUwAyyj441F",
    "outputId": "89a764c0-302a-46ae-de64-38eff088fac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: corner in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib>=2.1 in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from corner) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from matplotlib>=2.1->corner) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from matplotlib>=2.1->corner) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from matplotlib>=2.1->corner) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from matplotlib>=2.1->corner) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from matplotlib>=2.1->corner) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from matplotlib>=2.1->corner) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from matplotlib>=2.1->corner) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from matplotlib>=2.1->corner) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from matplotlib>=2.1->corner) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=2.1->corner) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torchdiffeq in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (0.2.3)\n",
      "Requirement already satisfied: torch>=1.3.0 in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from torchdiffeq) (2.1.0.dev20230801)\n",
      "Requirement already satisfied: scipy>=1.4.0 in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from torchdiffeq) (1.11.4)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from scipy>=1.4.0->torchdiffeq) (1.26.0)\n",
      "Requirement already satisfied: filelock in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from torch>=1.3.0->torchdiffeq) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from torch>=1.3.0->torchdiffeq) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from torch>=1.3.0->torchdiffeq) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from torch>=1.3.0->torchdiffeq) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from torch>=1.3.0->torchdiffeq) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from torch>=1.3.0->torchdiffeq) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from jinja2->torch>=1.3.0->torchdiffeq) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/nicolehartman/miniconda3/lib/python3.11/site-packages (from sympy->torch>=1.3.0->torchdiffeq) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Installs if you're running on collab\n",
    "!pip install corner\n",
    "!pip install torchdiffeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4281,
     "status": "ok",
     "timestamp": 1727147259304,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "Rr_niqmV4t56"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Union\n",
    "import corner\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1727147259305,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "zSQuS-VB4t58"
   },
   "outputs": [],
   "source": [
    "# If running on CUDA is available, use the GPU.\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = \"mps\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1727147259305,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "Uy88NdeU4t59",
    "outputId": "1c3218b7-e9bb-4630-bdcf-30ffb4db1dca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qvy_ryTa4t5-"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "Here we will again load and preprocess the data we generated in the notebook `data_generation.ipynb` and saved under `01_normalizing_flows/data/training_dataset.pkl`.\n",
    "\n",
    "However, you can also save the file to your google drive (and might have already done so in the last couple of days) so the code block below toggles `data_folder` to load from the relevant location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1987,
     "status": "ok",
     "timestamp": 1727147261288,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "kDvG1Hsf5IHK",
    "outputId": "395c56a1-2497-4eaf-85b3-8f0daff29108"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Warning: Might need to edit based on where the data is stored.\n",
    "'''\n",
    "try:\n",
    "    # Try to load the google drive for google collab if you can\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    data_folder=\"/content/drive/My Drive\"\n",
    "    \n",
    "except:\n",
    "    # Just look for the folder locally\n",
    "    data_folder = '../01_normalizing_flows/data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 768,
     "status": "ok",
     "timestamp": 1727147262054,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "e9h4rPqc4t5_",
    "outputId": "af5bcd73-13e7-46c0-9a88-8e17ca7aa802"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucessfully loaded dataset with 10000 waveforms.\n"
     ]
    }
   ],
   "source": [
    "file_name = os.path.join(data_folder, 'dataset.pkl')\n",
    "\n",
    "if not os.path.isfile(file_name):\n",
    "    raise ValueError(f\"File {file_name} does not exist, correct path or generate data set.\")\n",
    "\n",
    "with open(file_name, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "print('Sucessfully loaded dataset with', len(data['hp']), 'waveforms.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbMaObiQ4t5_"
   },
   "source": [
    "Load in a pytorch Dataset and do the dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1727147262054,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "Bj7RrwKc4t6A"
   },
   "outputs": [],
   "source": [
    "class WaveformDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        '''\n",
    "        - data: As loaded from what we've saved from the\n",
    "                data_generation nb\n",
    "        '''\n",
    "\n",
    "        parameters = torch.FloatTensor(data['masses'])\n",
    "\n",
    "        # Standardize masses for training\n",
    "        self.parameters_mean = torch.mean(parameters, axis=0)\n",
    "        self.parameters_std = torch.std(parameters, axis=0)\n",
    "\n",
    "        parameters_standardized = (parameters - self.parameters_mean) / self.parameters_std\n",
    "\n",
    "        '''\n",
    "        Pre-process waveforms:\n",
    "        - truncate below f_min\n",
    "        - re-package real and imaginary part\n",
    "        - only use $h_+$ for simplicity\n",
    "        '''\n",
    "        hp = torch.tensor(data['hp'])\n",
    "        f_min, T = data['f_min'], data['T']\n",
    "        delta_f = 1/T\n",
    "\n",
    "        lower_cut = int(f_min / delta_f)\n",
    "        waveforms = torch.hstack([hp.real[:, lower_cut:], hp.imag[:, lower_cut:]])\n",
    "\n",
    "        self.parameters = parameters_standardized\n",
    "        self.waveforms = waveforms.float() # type-cast to torch.float32\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.parameters)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Returns:\n",
    "        params, t, cond_inputs (=data)\n",
    "        [which we'll be calling x,t,y in the code]\n",
    "        '''\n",
    "        params = self.parameters[idx]\n",
    "        signal = self.waveforms[idx]\n",
    "\n",
    "        # Add unit normal noise to the signal\n",
    "        noise = torch.randn(size = signal.shape)\n",
    "        data = signal + noise\n",
    "\n",
    "        return params.to(device), data.to(device)\n",
    "\n",
    "# Returns data in the form (params, data)\n",
    "waveform_dataset = WaveformDataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8DWqyW9E5jM"
   },
   "source": [
    "**Note:** The notation we'll use today is the same as the notation in the FM tutorial from yday, and also consistent w/ Eva's lecture.\n",
    "\n",
    "The `waveform_dataset` will return data in the form: $x,y$ to model the posterior $p_\\theta (x_1 | y)$ where\n",
    "\n",
    "- $x$ = $x_1$: `params`  GW masses $\\in \\mathbb{R}^2$\n",
    "- $y$: `data` we'll condition on, the $h_p$ strain signal\n",
    "\n",
    "$x_t$ is the state space that gets transformed by the diffusion and $y$ is the conditioning vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q77bc5oR4t6A"
   },
   "source": [
    "### Split data into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1727148613482,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "vyCuaP-L4t6B"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "num_samples = len(waveform_dataset)\n",
    "train_fraction = 0.8\n",
    "num_train = int(round(train_fraction * num_samples))\n",
    "num_test = num_samples - num_train\n",
    "train_dataset, test_dataset = random_split(waveform_dataset, [num_train, num_test])\n",
    "\n",
    "# The DataLoader is used in training\n",
    "\n",
    "batch_size=64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dim = x.shape[1]\n",
    "context_dim = y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Y038Gtm4t6B"
   },
   "source": [
    "## 2. Defining the model <a name=\"velocity-NN\"></a>\n",
    "\n",
    "### Forward process\n",
    "\n",
    "**From the lecture:**\n",
    "\n",
    "The conditional probability distribution is a next vector prediction which is a Guassian w/ the mean scaled down by $\\sqrt{1 - \\beta_t}$ and the variance set to $\\sqrt{\\beta_t}$\n",
    ".\n",
    "$$q(x_t | x_{t-1}) = \\mathcal{N}(x_t;\\sqrt{1-\\beta_t} x_{t-1}, \\sqrt{\\beta_t})$$\n",
    "\n",
    "$$x_t= \\sqrt{1-\\beta_t} x_{t-1}+ \\sqrt{\\beta_t} \\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0,1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could start from x_0 and _marginalize_ over the latent variables $x_1, x_2 , \\ldots, x_{t-1}$ to arrive at the conditional distribution\n",
    "\n",
    "$$x(t | x_0) = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon$$\n",
    "\n",
    "which is _also Gaussian_ with the new variable $\\bar{\\alpha}_t = (1-\\beta_0)(1-\\beta_1)\\cdots(1-\\beta_t)$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choices we need to make**\n",
    "\n",
    "1) What to choose for $\\beta_t$\n",
    "\n",
    "As you can see in the first equation above $\\sqrt{\\beta_t}$ is how much the noise gets scaled by at every step. For the tutorial, we'll just choose a linear $\\beta$ schedule but note the choice of **noise schedule**  has been the subjet of a lot of R&D in recent years! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(\n",
    "    n_steps: int   # number of time steps\n",
    ") -> torch.Tensor: # beta values at the time steps, shape (n_steps, )\n",
    "    scale = 1000 / n_steps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return torch.linspace(beta_start, beta_end, n_steps, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) How many time steps to make\n",
    "\n",
    "- Let's just go w/ 100.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classic \"gotcha\": It's easier to implement the code if $t \\in \\mathbb{N}$ so we can index lists w/ $t$ instead of having $t\\in [0,1]$ as yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 param_dim: int, \n",
    "                 context_dim: int = None,\n",
    "                 n_steps: int = 1000,    # number of time steps\n",
    "                 hidden_dim: int = 64, # TODO           \n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.param_dim = param_dim\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "        # Initialize alpha, beta and sigma constants for the given number of time steps\n",
    "        self.betas = linear_beta_schedule(n_steps)\n",
    "\n",
    "        alphas = 1 - self.betas\n",
    "        alphas_bar = torch.cumprod(alphas, dim=0)\n",
    "        \n",
    "        self.sqrt_alphas_bar = torch.sqrt(alphas_bar)\n",
    "        self.sqrt_one_minus_alphas_bar = torch.sqrt(1 - alphas_bar)\n",
    "        self.sigmas = torch.sqrt(self.betas)\n",
    "        '''\n",
    "        TASK: Define a small NN to predict the noise\n",
    "        '''\n",
    "        self.net = nn.Sequential(\n",
    "            ... # your code here\n",
    "        )\n",
    "    \n",
    "    def get_xt(self,x0,t,eps):\n",
    "        '''\n",
    "        Inputs:\n",
    "        - x0\n",
    "        - t\n",
    "        - eps: noise\n",
    "        \n",
    "        Q) Your task: Calculate x_t        \n",
    "        '''\n",
    "        raise Not ImplementedError\n",
    "        return xt         \n",
    "\n",
    "    def get_mu(self,xt,t,y):\n",
    "        '''\n",
    "        Predict the _mean_ of the reverse process.\n",
    "        Needed in the sample function\n",
    "\n",
    "        YOUR TASK: IMPLEMENT\n",
    "        '''\n",
    "        raise Not ImplementedError\n",
    "        return xt \n",
    "    \n",
    "    def sample(\n",
    "        self,\n",
    "        y: torch.Tensor,       # The context vector\n",
    "        keep_xt: bool = False, # whether to keep the intermediate x_t\n",
    "    ) -> torch.Tensor:         # sampled data, shape (n_samples, param_dim) or (n_steps, n_samples, param_dim)\n",
    "        '''\n",
    "        Defined in backup slides of Eva's lecture\n",
    "        '''\n",
    "        n_samples = y.shape[0]\n",
    "        xt = torch.randn(n_samples, self.param_dim)\n",
    "        if keep_xt:\n",
    "            xts = torch.zeros((n_samples, self.n_steps + 1, self.param_dim))\n",
    "            xts[:, self.n_steps] = xt\n",
    "        for t in reversed(range(1,self.n_steps)):\n",
    "            \n",
    "            z = torch.randn(n_samples, self.param_dim) if t > 1 else 0.\n",
    "\n",
    "            eps_pred = self.net(torch.cat([xt, torch.full((xt.shape[0], 1), t, dtype=torch.float32), y], dim=1))\n",
    "\n",
    "            x0_tilde = (1/self.sqrt_alphas_bar[t])*(xt - self.sqrt_one_minus_alphas_bar[t]*eps_pred)\n",
    "            mu_tilde = self.get_mu(xt,t,y)\n",
    "            xt = mu_tilde + self.sigmas[t] * z\n",
    "            if keep_xt:\n",
    "                xts[:, t] = x\n",
    "        return xts if keep_xt else xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(*x.shape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DDPM(param_dim,context_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.n_steps=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.5649,  3.7549],\n",
       "        [ 3.4135,  9.9429],\n",
       "        [-9.2171, -7.7231]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sample(torch.randn(3,context_dim)) #.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGZ_Mjjp4t6E"
   },
   "source": [
    "## 3. The Loss function + training <a name=\"velocity-NN\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losses(model, x0, y):\n",
    "    '''\n",
    "    Q) Fill out this function\n",
    "    '''\n",
    "\n",
    "    # Sample t as how many integer steps to take from 1 -> n_steps\n",
    "    t = torch.randint(low=1,high=model.n_steps,size=(x0.shape[0],1)).to(device)\n",
    "    \n",
    "    # Sample noise (same shape as x0)\n",
    "    eps = torch.randn(*x0.shape)\n",
    "\n",
    "    xt = model.get_xt(x0,t,eps)\n",
    "    eps_pred = model.net(\n",
    "        # TO DO: What all do you need to pass to the model(?)\n",
    "    )\n",
    "\n",
    "    return torch.sum((eps - eps_pred)**2,axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdlv9OM_4t6E"
   },
   "source": [
    "Then the train and test loops are the same as yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 1189,
     "status": "ok",
     "timestamp": 1727148810087,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "D2dlU00D4t6E"
   },
   "outputs": [],
   "source": [
    "# Training and test loops\n",
    "def train_loop(dataloader, model, optimizer):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch, (x,y) in enumerate(dataloader):\n",
    "\n",
    "        loss = get_losses(model,x,y)\n",
    "\n",
    "        train_loss += loss.detach().sum()\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            loss, current = loss.item(), batch * len(x)\n",
    "            print(f\"Loss: {loss:>7f}  [{current:>5d}/{size:>5d} samples]\")\n",
    "\n",
    "    average_loss = train_loss.item() / size\n",
    "    print('Average loss: {:.4f}'.format(average_loss))\n",
    "    return average_loss\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x,y in dataloader:\n",
    "            loss = get_losses(model,x,y)\n",
    "            test_loss += loss.sum()\n",
    "\n",
    "    test_loss = test_loss.item() / size\n",
    "    print(f\"Test loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 619089,
     "status": "ok",
     "timestamp": 1727147883090,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "0KdTjnNt4t6F",
    "outputId": "a30a340b-12c6-44d5-bb6b-822b07ca1cea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Loss: 2.028998  [    0/ 8000 samples]\n",
      "Loss: 1.954532  [ 3200/ 8000 samples]\n",
      "Loss: 1.931126  [ 6400/ 8000 samples]\n",
      "Average loss: 2.0019\n",
      "Test loss: 2.085334 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Loss: 2.390031  [    0/ 8000 samples]\n",
      "Loss: 2.533414  [ 3200/ 8000 samples]\n",
      "Loss: 1.527034  [ 6400/ 8000 samples]\n",
      "Average loss: 1.9860\n",
      "Test loss: 2.038891 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Loss: 1.745568  [    0/ 8000 samples]\n",
      "Loss: 2.740499  [ 3200/ 8000 samples]\n",
      "Loss: 1.422827  [ 6400/ 8000 samples]\n",
      "Average loss: 2.0108\n",
      "Test loss: 2.019931 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Loss: 1.960053  [    0/ 8000 samples]\n",
      "Loss: 2.258984  [ 3200/ 8000 samples]\n",
      "Loss: 2.227179  [ 6400/ 8000 samples]\n",
      "Average loss: 2.0177\n",
      "Test loss: 2.097908 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Loss: 2.520104  [    0/ 8000 samples]\n",
      "Loss: 2.217698  [ 3200/ 8000 samples]\n",
      "Loss: 1.705775  [ 6400/ 8000 samples]\n",
      "Average loss: 2.0200\n",
      "Test loss: 1.991115 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Loss: 1.737832  [    0/ 8000 samples]\n",
      "Loss: 1.895968  [ 3200/ 8000 samples]\n",
      "Loss: 2.148442  [ 6400/ 8000 samples]\n",
      "Average loss: 1.9860\n",
      "Test loss: 2.019643 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Loss: 1.855446  [    0/ 8000 samples]\n",
      "Loss: 1.616618  [ 3200/ 8000 samples]\n",
      "Loss: 1.841995  [ 6400/ 8000 samples]\n",
      "Average loss: 2.0027\n",
      "Test loss: 2.063667 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Loss: 2.117568  [    0/ 8000 samples]\n",
      "Loss: 1.813613  [ 3200/ 8000 samples]\n",
      "Loss: 2.101682  [ 6400/ 8000 samples]\n",
      "Average loss: 2.0028\n",
      "Test loss: 1.990081 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Loss: 1.851194  [    0/ 8000 samples]\n",
      "Loss: 1.915459  [ 3200/ 8000 samples]\n",
      "Loss: 2.152933  [ 6400/ 8000 samples]\n",
      "Average loss: 2.0353\n",
      "Test loss: 2.014675 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Loss: 1.972676  [    0/ 8000 samples]\n",
      "Loss: 2.002873  [ 3200/ 8000 samples]\n",
      "Loss: 2.077347  [ 6400/ 8000 samples]\n",
      "Average loss: 2.0089\n",
      "Test loss: 2.021667 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Loss: 2.031078  [    0/ 8000 samples]\n",
      "Loss: 1.354000  [ 3200/ 8000 samples]\n",
      "Loss: 1.809891  [ 6400/ 8000 samples]\n",
      "Average loss: 2.0006\n",
      "Test loss: 1.992804 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Loss: 1.843638  [    0/ 8000 samples]\n",
      "Loss: 2.298540  [ 3200/ 8000 samples]\n",
      "Loss: 1.627860  [ 6400/ 8000 samples]\n",
      "Average loss: 2.0113\n",
      "Test loss: 1.943403 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Loss: 2.058963  [    0/ 8000 samples]\n",
      "Loss: 1.690771  [ 3200/ 8000 samples]\n",
      "Loss: 2.433274  [ 6400/ 8000 samples]\n",
      "Average loss: 1.9908\n",
      "Test loss: 2.024748 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Loss: 1.681859  [    0/ 8000 samples]\n",
      "Loss: 2.233306  [ 3200/ 8000 samples]\n",
      "Loss: 2.093666  [ 6400/ 8000 samples]\n",
      "Average loss: 2.0095\n",
      "Test loss: 2.029421 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Loss: 1.869061  [    0/ 8000 samples]\n",
      "Loss: 2.366630  [ 3200/ 8000 samples]\n",
      "Loss: 1.642383  [ 6400/ 8000 samples]\n",
      "Average loss: 1.9855\n",
      "Test loss: 2.005730 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Loss: 2.224860  [    0/ 8000 samples]\n",
      "Loss: 1.619373  [ 3200/ 8000 samples]\n",
      "Loss: 1.775896  [ 6400/ 8000 samples]\n",
      "Average loss: 2.0298\n",
      "Test loss: 2.027532 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Loss: 1.632907  [    0/ 8000 samples]\n",
      "Loss: 1.706891  [ 3200/ 8000 samples]\n",
      "Loss: 2.185560  [ 6400/ 8000 samples]\n",
      "Average loss: 1.9767\n",
      "Test loss: 1.989712 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Loss: 2.024406  [    0/ 8000 samples]\n",
      "Loss: 1.818394  [ 3200/ 8000 samples]\n",
      "Loss: 2.315134  [ 6400/ 8000 samples]\n",
      "Average loss: 2.0026\n",
      "Test loss: 2.040398 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Loss: 1.922043  [    0/ 8000 samples]\n",
      "Loss: 2.082535  [ 3200/ 8000 samples]\n",
      "Loss: 2.233361  [ 6400/ 8000 samples]\n",
      "Average loss: 1.9979\n",
      "Test loss: 1.989751 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Loss: 2.296678  [    0/ 8000 samples]\n",
      "Loss: 1.579689  [ 3200/ 8000 samples]\n",
      "Loss: 2.236350  [ 6400/ 8000 samples]\n",
      "Average loss: 1.9941\n",
      "Test loss: 2.067897 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Loss: 2.316738  [    0/ 8000 samples]\n",
      "Loss: 1.829935  [ 3200/ 8000 samples]\n",
      "Loss: 2.333744  [ 6400/ 8000 samples]\n",
      "Average loss: 2.0205\n",
      "Test loss: 1.952126 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Loss: 2.127878  [    0/ 8000 samples]\n",
      "Loss: 2.136374  [ 3200/ 8000 samples]\n",
      "Loss: 2.122954  [ 6400/ 8000 samples]\n",
      "Average loss: 1.9939\n",
      "Test loss: 2.045163 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Loss: 2.054722  [    0/ 8000 samples]\n",
      "Loss: 1.829882  [ 3200/ 8000 samples]\n",
      "Loss: 2.009023  [ 6400/ 8000 samples]\n",
      "Average loss: 1.9969\n",
      "Test loss: 1.948334 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Loss: 2.281203  [    0/ 8000 samples]\n",
      "Loss: 1.806006  [ 3200/ 8000 samples]\n",
      "Loss: 2.124774  [ 6400/ 8000 samples]\n",
      "Average loss: 1.9584\n",
      "Test loss: 1.892964 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Loss: 1.681412  [    0/ 8000 samples]\n",
      "Loss: 2.200706  [ 3200/ 8000 samples]\n",
      "Loss: 1.470264  [ 6400/ 8000 samples]\n",
      "Average loss: 1.9359\n",
      "Test loss: 2.030813 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Loss: 1.766170  [    0/ 8000 samples]\n",
      "Loss: 1.748154  [ 3200/ 8000 samples]\n",
      "Loss: 1.838518  [ 6400/ 8000 samples]\n",
      "Average loss: 1.9773\n",
      "Test loss: 2.003754 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Loss: 2.277924  [    0/ 8000 samples]\n",
      "Loss: 2.244176  [ 3200/ 8000 samples]\n",
      "Loss: 1.695227  [ 6400/ 8000 samples]\n",
      "Average loss: 1.9642\n",
      "Test loss: 1.972196 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Loss: 1.919498  [    0/ 8000 samples]\n",
      "Loss: 1.662479  [ 3200/ 8000 samples]\n",
      "Loss: 1.826530  [ 6400/ 8000 samples]\n",
      "Average loss: 1.9774\n",
      "Test loss: 1.921169 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Loss: 1.766291  [    0/ 8000 samples]\n",
      "Loss: 1.682230  [ 3200/ 8000 samples]\n",
      "Loss: 1.688241  [ 6400/ 8000 samples]\n",
      "Average loss: 1.9160\n",
      "Test loss: 1.951293 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Loss: 1.914766  [    0/ 8000 samples]\n",
      "Loss: 1.961300  [ 3200/ 8000 samples]\n",
      "Loss: 1.792911  [ 6400/ 8000 samples]\n",
      "Average loss: 1.8637\n",
      "Test loss: 1.880445 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Loss: 1.671723  [    0/ 8000 samples]\n",
      "Loss: 2.106479  [ 3200/ 8000 samples]\n",
      "Loss: 2.299272  [ 6400/ 8000 samples]\n",
      "Average loss: 1.9308\n",
      "Test loss: 1.844957 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Loss: 1.850176  [    0/ 8000 samples]\n",
      "Loss: 2.138741  [ 3200/ 8000 samples]\n",
      "Loss: 2.009402  [ 6400/ 8000 samples]\n",
      "Average loss: 1.8883\n",
      "Test loss: 1.833293 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Loss: 2.129086  [    0/ 8000 samples]\n",
      "Loss: 1.967440  [ 3200/ 8000 samples]\n",
      "Loss: 1.731182  [ 6400/ 8000 samples]\n",
      "Average loss: 1.8579\n",
      "Test loss: 1.766764 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Loss: 1.791442  [    0/ 8000 samples]\n",
      "Loss: 1.732372  [ 3200/ 8000 samples]\n",
      "Loss: 2.025415  [ 6400/ 8000 samples]\n",
      "Average loss: 1.7676\n",
      "Test loss: 1.750545 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Loss: 1.282000  [    0/ 8000 samples]\n",
      "Loss: 1.537227  [ 3200/ 8000 samples]\n",
      "Loss: 1.492335  [ 6400/ 8000 samples]\n",
      "Average loss: 1.7711\n",
      "Test loss: 1.706336 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Loss: 1.897584  [    0/ 8000 samples]\n",
      "Loss: 1.489839  [ 3200/ 8000 samples]\n",
      "Loss: 2.123849  [ 6400/ 8000 samples]\n",
      "Average loss: 1.7448\n",
      "Test loss: 1.691731 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Loss: 1.606161  [    0/ 8000 samples]\n",
      "Loss: 1.839341  [ 3200/ 8000 samples]\n",
      "Loss: 1.734481  [ 6400/ 8000 samples]\n",
      "Average loss: 1.6676\n",
      "Test loss: 1.665183 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Loss: 1.568288  [    0/ 8000 samples]\n",
      "Loss: 1.978184  [ 3200/ 8000 samples]\n",
      "Loss: 1.943172  [ 6400/ 8000 samples]\n",
      "Average loss: 1.6434\n",
      "Test loss: 1.667429 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Loss: 1.433876  [    0/ 8000 samples]\n",
      "Loss: 1.459806  [ 3200/ 8000 samples]\n",
      "Loss: 1.530685  [ 6400/ 8000 samples]\n",
      "Average loss: 1.6014\n",
      "Test loss: 1.559338 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Loss: 1.808343  [    0/ 8000 samples]\n",
      "Loss: 1.462529  [ 3200/ 8000 samples]\n",
      "Loss: 1.392728  [ 6400/ 8000 samples]\n",
      "Average loss: 1.6042\n",
      "Test loss: 1.593121 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Loss: 1.355236  [    0/ 8000 samples]\n",
      "Loss: 1.743441  [ 3200/ 8000 samples]\n",
      "Loss: 1.897789  [ 6400/ 8000 samples]\n",
      "Average loss: 1.5131\n",
      "Test loss: 1.450505 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Loss: 1.469974  [    0/ 8000 samples]\n",
      "Loss: 1.300803  [ 3200/ 8000 samples]\n",
      "Loss: 1.655128  [ 6400/ 8000 samples]\n",
      "Average loss: 1.5192\n",
      "Test loss: 1.376282 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Loss: 1.437968  [    0/ 8000 samples]\n",
      "Loss: 1.341166  [ 3200/ 8000 samples]\n",
      "Loss: 1.382426  [ 6400/ 8000 samples]\n",
      "Average loss: 1.4153\n",
      "Test loss: 1.475893 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Loss: 1.202473  [    0/ 8000 samples]\n",
      "Loss: 1.459235  [ 3200/ 8000 samples]\n",
      "Loss: 1.359089  [ 6400/ 8000 samples]\n",
      "Average loss: 1.3911\n",
      "Test loss: 1.326848 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Loss: 1.705116  [    0/ 8000 samples]\n",
      "Loss: 1.008925  [ 3200/ 8000 samples]\n",
      "Loss: 1.174243  [ 6400/ 8000 samples]\n",
      "Average loss: 1.3759\n",
      "Test loss: 1.327293 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Loss: 1.202033  [    0/ 8000 samples]\n",
      "Loss: 1.529255  [ 3200/ 8000 samples]\n",
      "Loss: 1.309073  [ 6400/ 8000 samples]\n",
      "Average loss: 1.3116\n",
      "Test loss: 1.265750 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Loss: 1.332437  [    0/ 8000 samples]\n",
      "Loss: 1.280540  [ 3200/ 8000 samples]\n",
      "Loss: 1.197196  [ 6400/ 8000 samples]\n",
      "Average loss: 1.2833\n",
      "Test loss: 1.277962 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Loss: 1.129749  [    0/ 8000 samples]\n",
      "Loss: 1.389827  [ 3200/ 8000 samples]\n",
      "Loss: 1.289426  [ 6400/ 8000 samples]\n",
      "Average loss: 1.2575\n",
      "Test loss: 1.323545 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Loss: 1.431308  [    0/ 8000 samples]\n",
      "Loss: 1.442271  [ 3200/ 8000 samples]\n",
      "Loss: 1.257625  [ 6400/ 8000 samples]\n",
      "Average loss: 1.2543\n",
      "Test loss: 1.225300 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Loss: 1.334746  [    0/ 8000 samples]\n",
      "Loss: 1.353694  [ 3200/ 8000 samples]\n",
      "Loss: 1.299471  [ 6400/ 8000 samples]\n",
      "Average loss: 1.1889\n",
      "Test loss: 1.145254 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Loss: 1.097353  [    0/ 8000 samples]\n",
      "Loss: 1.190227  [ 3200/ 8000 samples]\n",
      "Loss: 0.999053  [ 6400/ 8000 samples]\n",
      "Average loss: 1.1610\n",
      "Test loss: 1.104421 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Loss: 1.239177  [    0/ 8000 samples]\n",
      "Loss: 1.196942  [ 3200/ 8000 samples]\n",
      "Loss: 0.985915  [ 6400/ 8000 samples]\n",
      "Average loss: 1.1451\n",
      "Test loss: 1.112812 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Loss: 0.992893  [    0/ 8000 samples]\n",
      "Loss: 1.013739  [ 3200/ 8000 samples]\n",
      "Loss: 0.916663  [ 6400/ 8000 samples]\n",
      "Average loss: 1.1353\n",
      "Test loss: 1.053638 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Loss: 1.069544  [    0/ 8000 samples]\n",
      "Loss: 0.844479  [ 3200/ 8000 samples]\n",
      "Loss: 1.262383  [ 6400/ 8000 samples]\n",
      "Average loss: 1.0784\n",
      "Test loss: 1.027930 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Loss: 0.957461  [    0/ 8000 samples]\n",
      "Loss: 1.233726  [ 3200/ 8000 samples]\n",
      "Loss: 1.269499  [ 6400/ 8000 samples]\n",
      "Average loss: 1.0641\n",
      "Test loss: 1.082794 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Loss: 1.323581  [    0/ 8000 samples]\n",
      "Loss: 0.777758  [ 3200/ 8000 samples]\n",
      "Loss: 1.275604  [ 6400/ 8000 samples]\n",
      "Average loss: 1.0982\n",
      "Test loss: 1.052146 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Loss: 1.009651  [    0/ 8000 samples]\n",
      "Loss: 0.893026  [ 3200/ 8000 samples]\n",
      "Loss: 1.046318  [ 6400/ 8000 samples]\n",
      "Average loss: 1.0703\n",
      "Test loss: 1.068117 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Loss: 0.964400  [    0/ 8000 samples]\n",
      "Loss: 1.010291  [ 3200/ 8000 samples]\n",
      "Loss: 1.262196  [ 6400/ 8000 samples]\n",
      "Average loss: 1.0742\n",
      "Test loss: 1.022245 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Loss: 1.153912  [    0/ 8000 samples]\n",
      "Loss: 0.959389  [ 3200/ 8000 samples]\n",
      "Loss: 1.200527  [ 6400/ 8000 samples]\n",
      "Average loss: 1.0024\n",
      "Test loss: 1.045727 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Loss: 1.006065  [    0/ 8000 samples]\n",
      "Loss: 1.358623  [ 3200/ 8000 samples]\n",
      "Loss: 0.886801  [ 6400/ 8000 samples]\n",
      "Average loss: 0.9954\n",
      "Test loss: 1.046422 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Loss: 1.352031  [    0/ 8000 samples]\n",
      "Loss: 0.986347  [ 3200/ 8000 samples]\n",
      "Loss: 0.795204  [ 6400/ 8000 samples]\n",
      "Average loss: 0.9441\n",
      "Test loss: 0.972803 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Loss: 1.053127  [    0/ 8000 samples]\n",
      "Loss: 0.966367  [ 3200/ 8000 samples]\n",
      "Loss: 1.352035  [ 6400/ 8000 samples]\n",
      "Average loss: 1.0217\n",
      "Test loss: 0.955418 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Loss: 0.732318  [    0/ 8000 samples]\n",
      "Loss: 0.971924  [ 3200/ 8000 samples]\n",
      "Loss: 0.782514  [ 6400/ 8000 samples]\n",
      "Average loss: 0.9278\n",
      "Test loss: 0.993990 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Loss: 1.197419  [    0/ 8000 samples]\n",
      "Loss: 0.959051  [ 3200/ 8000 samples]\n",
      "Loss: 0.781814  [ 6400/ 8000 samples]\n",
      "Average loss: 0.9698\n",
      "Test loss: 0.965591 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Loss: 1.062166  [    0/ 8000 samples]\n",
      "Loss: 1.242269  [ 3200/ 8000 samples]\n",
      "Loss: 1.076533  [ 6400/ 8000 samples]\n",
      "Average loss: 0.9915\n",
      "Test loss: 0.974055 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Loss: 1.039039  [    0/ 8000 samples]\n",
      "Loss: 0.896028  [ 3200/ 8000 samples]\n",
      "Loss: 0.885185  [ 6400/ 8000 samples]\n",
      "Average loss: 0.9851\n",
      "Test loss: 0.982057 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Loss: 0.760685  [    0/ 8000 samples]\n",
      "Loss: 1.027005  [ 3200/ 8000 samples]\n",
      "Loss: 1.107551  [ 6400/ 8000 samples]\n",
      "Average loss: 0.9309\n",
      "Test loss: 0.901787 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Loss: 0.842099  [    0/ 8000 samples]\n",
      "Loss: 0.938879  [ 3200/ 8000 samples]\n",
      "Loss: 0.849966  [ 6400/ 8000 samples]\n",
      "Average loss: 0.9530\n",
      "Test loss: 0.937309 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Loss: 0.928558  [    0/ 8000 samples]\n",
      "Loss: 0.772795  [ 3200/ 8000 samples]\n",
      "Loss: 1.050282  [ 6400/ 8000 samples]\n",
      "Average loss: 0.9197\n",
      "Test loss: 0.943204 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Loss: 0.768537  [    0/ 8000 samples]\n",
      "Loss: 0.792707  [ 3200/ 8000 samples]\n",
      "Loss: 0.972548  [ 6400/ 8000 samples]\n",
      "Average loss: 0.9221\n",
      "Test loss: 0.867630 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Loss: 0.937567  [    0/ 8000 samples]\n",
      "Loss: 1.118561  [ 3200/ 8000 samples]\n",
      "Loss: 0.944385  [ 6400/ 8000 samples]\n",
      "Average loss: 0.9311\n",
      "Test loss: 0.931472 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Loss: 1.145102  [    0/ 8000 samples]\n",
      "Loss: 1.128244  [ 3200/ 8000 samples]\n",
      "Loss: 0.700696  [ 6400/ 8000 samples]\n",
      "Average loss: 0.9472\n",
      "Test loss: 0.965062 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Loss: 0.973716  [    0/ 8000 samples]\n",
      "Loss: 1.025531  [ 3200/ 8000 samples]\n",
      "Loss: 0.731304  [ 6400/ 8000 samples]\n",
      "Average loss: 0.9063\n",
      "Test loss: 0.889968 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Loss: 0.705935  [    0/ 8000 samples]\n",
      "Loss: 0.974570  [ 3200/ 8000 samples]\n",
      "Loss: 1.035733  [ 6400/ 8000 samples]\n",
      "Average loss: 0.9011\n",
      "Test loss: 0.877513 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Loss: 0.557703  [    0/ 8000 samples]\n",
      "Loss: 1.099111  [ 3200/ 8000 samples]\n",
      "Loss: 0.889022  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8710\n",
      "Test loss: 0.965386 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Loss: 1.027522  [    0/ 8000 samples]\n",
      "Loss: 0.916842  [ 3200/ 8000 samples]\n",
      "Loss: 0.743559  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8992\n",
      "Test loss: 0.819924 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Loss: 0.809531  [    0/ 8000 samples]\n",
      "Loss: 0.786114  [ 3200/ 8000 samples]\n",
      "Loss: 1.112470  [ 6400/ 8000 samples]\n",
      "Average loss: 0.9049\n",
      "Test loss: 0.926240 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Loss: 0.931924  [    0/ 8000 samples]\n",
      "Loss: 0.690794  [ 3200/ 8000 samples]\n",
      "Loss: 0.923988  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8887\n",
      "Test loss: 0.951590 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Loss: 0.753324  [    0/ 8000 samples]\n",
      "Loss: 0.826764  [ 3200/ 8000 samples]\n",
      "Loss: 0.813682  [ 6400/ 8000 samples]\n",
      "Average loss: 0.9126\n",
      "Test loss: 0.831770 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Loss: 0.898263  [    0/ 8000 samples]\n",
      "Loss: 0.885455  [ 3200/ 8000 samples]\n",
      "Loss: 0.885606  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8567\n",
      "Test loss: 0.844186 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Loss: 1.003139  [    0/ 8000 samples]\n",
      "Loss: 1.033058  [ 3200/ 8000 samples]\n",
      "Loss: 1.173085  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8601\n",
      "Test loss: 0.939766 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Loss: 1.019472  [    0/ 8000 samples]\n",
      "Loss: 0.888704  [ 3200/ 8000 samples]\n",
      "Loss: 0.643711  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8589\n",
      "Test loss: 0.874635 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Loss: 0.826441  [    0/ 8000 samples]\n",
      "Loss: 0.875357  [ 3200/ 8000 samples]\n",
      "Loss: 0.899043  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8568\n",
      "Test loss: 0.891690 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Loss: 1.072912  [    0/ 8000 samples]\n",
      "Loss: 0.820781  [ 3200/ 8000 samples]\n",
      "Loss: 0.783518  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8664\n",
      "Test loss: 0.885784 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Loss: 0.696192  [    0/ 8000 samples]\n",
      "Loss: 0.742677  [ 3200/ 8000 samples]\n",
      "Loss: 0.836200  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8513\n",
      "Test loss: 0.842079 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Loss: 0.688661  [    0/ 8000 samples]\n",
      "Loss: 0.934987  [ 3200/ 8000 samples]\n",
      "Loss: 0.915234  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8317\n",
      "Test loss: 0.791080 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Loss: 0.706979  [    0/ 8000 samples]\n",
      "Loss: 0.777898  [ 3200/ 8000 samples]\n",
      "Loss: 0.757108  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8276\n",
      "Test loss: 0.836491 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Loss: 1.116397  [    0/ 8000 samples]\n",
      "Loss: 0.903031  [ 3200/ 8000 samples]\n",
      "Loss: 0.806381  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8413\n",
      "Test loss: 0.826072 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Loss: 0.798446  [    0/ 8000 samples]\n",
      "Loss: 1.008838  [ 3200/ 8000 samples]\n",
      "Loss: 0.708323  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8469\n",
      "Test loss: 0.879948 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Loss: 0.724035  [    0/ 8000 samples]\n",
      "Loss: 0.862573  [ 3200/ 8000 samples]\n",
      "Loss: 0.846640  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8346\n",
      "Test loss: 0.815137 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Loss: 0.955015  [    0/ 8000 samples]\n",
      "Loss: 0.965839  [ 3200/ 8000 samples]\n",
      "Loss: 1.010669  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8171\n",
      "Test loss: 0.831499 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Loss: 0.678528  [    0/ 8000 samples]\n",
      "Loss: 0.759085  [ 3200/ 8000 samples]\n",
      "Loss: 0.714476  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8211\n",
      "Test loss: 0.871222 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Loss: 0.873286  [    0/ 8000 samples]\n",
      "Loss: 0.806122  [ 3200/ 8000 samples]\n",
      "Loss: 0.901687  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8215\n",
      "Test loss: 0.818780 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Loss: 0.702183  [    0/ 8000 samples]\n",
      "Loss: 1.000094  [ 3200/ 8000 samples]\n",
      "Loss: 0.728378  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8034\n",
      "Test loss: 0.866925 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Loss: 0.975587  [    0/ 8000 samples]\n",
      "Loss: 0.676074  [ 3200/ 8000 samples]\n",
      "Loss: 0.640687  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8006\n",
      "Test loss: 0.798960 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Loss: 0.787040  [    0/ 8000 samples]\n",
      "Loss: 1.003469  [ 3200/ 8000 samples]\n",
      "Loss: 0.745185  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7981\n",
      "Test loss: 0.806136 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Loss: 1.105117  [    0/ 8000 samples]\n",
      "Loss: 0.822551  [ 3200/ 8000 samples]\n",
      "Loss: 0.699358  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7926\n",
      "Test loss: 0.844227 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Loss: 0.806178  [    0/ 8000 samples]\n",
      "Loss: 0.961037  [ 3200/ 8000 samples]\n",
      "Loss: 0.794535  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8070\n",
      "Test loss: 0.852478 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Loss: 0.829601  [    0/ 8000 samples]\n",
      "Loss: 0.755497  [ 3200/ 8000 samples]\n",
      "Loss: 0.706607  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8078\n",
      "Test loss: 0.785084 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Loss: 0.760263  [    0/ 8000 samples]\n",
      "Loss: 0.775985  [ 3200/ 8000 samples]\n",
      "Loss: 0.801428  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7817\n",
      "Test loss: 0.768115 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Loss: 0.698282  [    0/ 8000 samples]\n",
      "Loss: 0.604491  [ 3200/ 8000 samples]\n",
      "Loss: 0.814713  [ 6400/ 8000 samples]\n",
      "Average loss: 0.8072\n",
      "Test loss: 0.744482 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Loss: 0.539512  [    0/ 8000 samples]\n",
      "Loss: 0.790738  [ 3200/ 8000 samples]\n",
      "Loss: 0.788291  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7888\n",
      "Test loss: 0.762175 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Loss: 0.687153  [    0/ 8000 samples]\n",
      "Loss: 0.631914  [ 3200/ 8000 samples]\n",
      "Loss: 0.563740  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7638\n",
      "Test loss: 0.784629 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Loss: 0.948859  [    0/ 8000 samples]\n",
      "Loss: 0.569515  [ 3200/ 8000 samples]\n",
      "Loss: 0.736846  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7776\n",
      "Test loss: 0.832034 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Loss: 0.936222  [    0/ 8000 samples]\n",
      "Loss: 0.780310  [ 3200/ 8000 samples]\n",
      "Loss: 0.836079  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7886\n",
      "Test loss: 0.761410 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Loss: 0.807545  [    0/ 8000 samples]\n",
      "Loss: 0.646505  [ 3200/ 8000 samples]\n",
      "Loss: 0.596968  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7614\n",
      "Test loss: 0.797858 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Loss: 0.922236  [    0/ 8000 samples]\n",
      "Loss: 0.578382  [ 3200/ 8000 samples]\n",
      "Loss: 0.811375  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7641\n",
      "Test loss: 0.781440 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Loss: 0.585012  [    0/ 8000 samples]\n",
      "Loss: 0.687821  [ 3200/ 8000 samples]\n",
      "Loss: 0.663903  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7641\n",
      "Test loss: 0.745163 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Loss: 0.839390  [    0/ 8000 samples]\n",
      "Loss: 0.877466  [ 3200/ 8000 samples]\n",
      "Loss: 0.673866  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7451\n",
      "Test loss: 0.754403 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Loss: 0.740227  [    0/ 8000 samples]\n",
      "Loss: 0.853017  [ 3200/ 8000 samples]\n",
      "Loss: 0.909446  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7632\n",
      "Test loss: 0.723590 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Loss: 0.758007  [    0/ 8000 samples]\n",
      "Loss: 0.745818  [ 3200/ 8000 samples]\n",
      "Loss: 0.787045  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7438\n",
      "Test loss: 0.748763 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Loss: 0.932279  [    0/ 8000 samples]\n",
      "Loss: 0.754329  [ 3200/ 8000 samples]\n",
      "Loss: 0.851962  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7650\n",
      "Test loss: 0.721028 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Loss: 0.576477  [    0/ 8000 samples]\n",
      "Loss: 0.661641  [ 3200/ 8000 samples]\n",
      "Loss: 1.127805  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7641\n",
      "Test loss: 0.733435 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Loss: 0.808050  [    0/ 8000 samples]\n",
      "Loss: 0.807051  [ 3200/ 8000 samples]\n",
      "Loss: 0.945003  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7863\n",
      "Test loss: 0.729485 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Loss: 0.774671  [    0/ 8000 samples]\n",
      "Loss: 0.936322  [ 3200/ 8000 samples]\n",
      "Loss: 0.765550  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7359\n",
      "Test loss: 0.800799 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Loss: 0.617371  [    0/ 8000 samples]\n",
      "Loss: 0.646743  [ 3200/ 8000 samples]\n",
      "Loss: 0.679952  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7541\n",
      "Test loss: 0.802502 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Loss: 0.945860  [    0/ 8000 samples]\n",
      "Loss: 0.618796  [ 3200/ 8000 samples]\n",
      "Loss: 0.492727  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7590\n",
      "Test loss: 0.695853 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Loss: 0.704327  [    0/ 8000 samples]\n",
      "Loss: 0.751584  [ 3200/ 8000 samples]\n",
      "Loss: 0.897766  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7603\n",
      "Test loss: 0.768152 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Loss: 0.755797  [    0/ 8000 samples]\n",
      "Loss: 0.737135  [ 3200/ 8000 samples]\n",
      "Loss: 0.819386  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7545\n",
      "Test loss: 0.752786 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Loss: 0.654246  [    0/ 8000 samples]\n",
      "Loss: 0.565736  [ 3200/ 8000 samples]\n",
      "Loss: 0.941862  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7633\n",
      "Test loss: 0.720346 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Loss: 0.906734  [    0/ 8000 samples]\n",
      "Loss: 0.882874  [ 3200/ 8000 samples]\n",
      "Loss: 0.684970  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7372\n",
      "Test loss: 0.742152 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Loss: 0.561847  [    0/ 8000 samples]\n",
      "Loss: 0.674365  [ 3200/ 8000 samples]\n",
      "Loss: 0.566868  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7420\n",
      "Test loss: 0.720760 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Loss: 0.785487  [    0/ 8000 samples]\n",
      "Loss: 0.621941  [ 3200/ 8000 samples]\n",
      "Loss: 0.764119  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7400\n",
      "Test loss: 0.749276 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Loss: 0.847273  [    0/ 8000 samples]\n",
      "Loss: 0.904521  [ 3200/ 8000 samples]\n",
      "Loss: 0.645102  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7466\n",
      "Test loss: 0.740888 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Loss: 0.684178  [    0/ 8000 samples]\n",
      "Loss: 0.911585  [ 3200/ 8000 samples]\n",
      "Loss: 0.656567  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7526\n",
      "Test loss: 0.732219 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Loss: 0.759476  [    0/ 8000 samples]\n",
      "Loss: 0.999572  [ 3200/ 8000 samples]\n",
      "Loss: 0.585205  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7495\n",
      "Test loss: 0.652712 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Loss: 0.819301  [    0/ 8000 samples]\n",
      "Loss: 0.519781  [ 3200/ 8000 samples]\n",
      "Loss: 0.646060  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7316\n",
      "Test loss: 0.725700 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Loss: 0.621335  [    0/ 8000 samples]\n",
      "Loss: 0.703707  [ 3200/ 8000 samples]\n",
      "Loss: 0.817333  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7510\n",
      "Test loss: 0.674148 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Loss: 0.401324  [    0/ 8000 samples]\n",
      "Loss: 0.771886  [ 3200/ 8000 samples]\n",
      "Loss: 0.565627  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7467\n",
      "Test loss: 0.737492 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Loss: 0.967330  [    0/ 8000 samples]\n",
      "Loss: 0.848342  [ 3200/ 8000 samples]\n",
      "Loss: 0.814543  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7544\n",
      "Test loss: 0.734142 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Loss: 0.652464  [    0/ 8000 samples]\n",
      "Loss: 0.692895  [ 3200/ 8000 samples]\n",
      "Loss: 0.889302  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7353\n",
      "Test loss: 0.682025 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Loss: 0.774119  [    0/ 8000 samples]\n",
      "Loss: 0.900699  [ 3200/ 8000 samples]\n",
      "Loss: 0.509632  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7388\n",
      "Test loss: 0.728465 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Loss: 0.666843  [    0/ 8000 samples]\n",
      "Loss: 0.546697  [ 3200/ 8000 samples]\n",
      "Loss: 0.873469  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7144\n",
      "Test loss: 0.706300 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Loss: 0.693643  [    0/ 8000 samples]\n",
      "Loss: 0.687929  [ 3200/ 8000 samples]\n",
      "Loss: 0.642478  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7052\n",
      "Test loss: 0.720262 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Loss: 0.566775  [    0/ 8000 samples]\n",
      "Loss: 0.712010  [ 3200/ 8000 samples]\n",
      "Loss: 0.574247  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7048\n",
      "Test loss: 0.730102 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Loss: 0.672397  [    0/ 8000 samples]\n",
      "Loss: 0.639915  [ 3200/ 8000 samples]\n",
      "Loss: 0.637106  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7155\n",
      "Test loss: 0.729599 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Loss: 1.047358  [    0/ 8000 samples]\n",
      "Loss: 0.832116  [ 3200/ 8000 samples]\n",
      "Loss: 0.557403  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7270\n",
      "Test loss: 0.713486 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Loss: 0.899242  [    0/ 8000 samples]\n",
      "Loss: 0.653249  [ 3200/ 8000 samples]\n",
      "Loss: 0.709140  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7025\n",
      "Test loss: 0.678268 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Loss: 0.696709  [    0/ 8000 samples]\n",
      "Loss: 0.409348  [ 3200/ 8000 samples]\n",
      "Loss: 0.805141  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7105\n",
      "Test loss: 0.797499 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Loss: 0.578694  [    0/ 8000 samples]\n",
      "Loss: 0.568780  [ 3200/ 8000 samples]\n",
      "Loss: 0.658148  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6813\n",
      "Test loss: 0.698349 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Loss: 0.706250  [    0/ 8000 samples]\n",
      "Loss: 0.642879  [ 3200/ 8000 samples]\n",
      "Loss: 0.641745  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7009\n",
      "Test loss: 0.782462 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Loss: 0.575433  [    0/ 8000 samples]\n",
      "Loss: 0.623120  [ 3200/ 8000 samples]\n",
      "Loss: 0.589373  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7208\n",
      "Test loss: 0.717452 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Loss: 0.752841  [    0/ 8000 samples]\n",
      "Loss: 0.851841  [ 3200/ 8000 samples]\n",
      "Loss: 0.815105  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6908\n",
      "Test loss: 0.672802 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Loss: 0.599186  [    0/ 8000 samples]\n",
      "Loss: 1.018585  [ 3200/ 8000 samples]\n",
      "Loss: 0.674290  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7090\n",
      "Test loss: 0.704271 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Loss: 0.914555  [    0/ 8000 samples]\n",
      "Loss: 1.042047  [ 3200/ 8000 samples]\n",
      "Loss: 0.671364  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7345\n",
      "Test loss: 0.830879 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Loss: 0.806942  [    0/ 8000 samples]\n",
      "Loss: 0.567737  [ 3200/ 8000 samples]\n",
      "Loss: 0.866295  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7012\n",
      "Test loss: 0.701079 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Loss: 0.529858  [    0/ 8000 samples]\n",
      "Loss: 0.528941  [ 3200/ 8000 samples]\n",
      "Loss: 0.631456  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7027\n",
      "Test loss: 0.713314 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Loss: 0.849461  [    0/ 8000 samples]\n",
      "Loss: 0.557730  [ 3200/ 8000 samples]\n",
      "Loss: 0.725968  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7043\n",
      "Test loss: 0.645848 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Loss: 0.885073  [    0/ 8000 samples]\n",
      "Loss: 0.515993  [ 3200/ 8000 samples]\n",
      "Loss: 0.752879  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7164\n",
      "Test loss: 0.689517 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Loss: 0.716121  [    0/ 8000 samples]\n",
      "Loss: 0.828776  [ 3200/ 8000 samples]\n",
      "Loss: 0.590027  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6939\n",
      "Test loss: 0.676492 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "Loss: 0.581171  [    0/ 8000 samples]\n",
      "Loss: 1.004418  [ 3200/ 8000 samples]\n",
      "Loss: 0.853528  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7030\n",
      "Test loss: 0.726512 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "Loss: 0.864188  [    0/ 8000 samples]\n",
      "Loss: 0.709244  [ 3200/ 8000 samples]\n",
      "Loss: 0.600723  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7057\n",
      "Test loss: 0.703234 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "Loss: 0.743398  [    0/ 8000 samples]\n",
      "Loss: 0.719717  [ 3200/ 8000 samples]\n",
      "Loss: 0.724072  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6988\n",
      "Test loss: 0.672557 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "Loss: 0.791781  [    0/ 8000 samples]\n",
      "Loss: 0.672708  [ 3200/ 8000 samples]\n",
      "Loss: 0.600990  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7071\n",
      "Test loss: 0.639786 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "Loss: 0.641008  [    0/ 8000 samples]\n",
      "Loss: 0.475164  [ 3200/ 8000 samples]\n",
      "Loss: 0.546255  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6860\n",
      "Test loss: 0.704497 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "Loss: 0.766873  [    0/ 8000 samples]\n",
      "Loss: 0.613569  [ 3200/ 8000 samples]\n",
      "Loss: 0.780640  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6843\n",
      "Test loss: 0.654378 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "Loss: 0.627588  [    0/ 8000 samples]\n",
      "Loss: 0.749478  [ 3200/ 8000 samples]\n",
      "Loss: 0.783057  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6883\n",
      "Test loss: 0.769616 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "Loss: 0.665875  [    0/ 8000 samples]\n",
      "Loss: 0.606900  [ 3200/ 8000 samples]\n",
      "Loss: 0.739739  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7065\n",
      "Test loss: 0.753845 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "Loss: 0.787896  [    0/ 8000 samples]\n",
      "Loss: 0.711060  [ 3200/ 8000 samples]\n",
      "Loss: 0.850528  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6988\n",
      "Test loss: 0.667904 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "Loss: 0.542173  [    0/ 8000 samples]\n",
      "Loss: 0.676888  [ 3200/ 8000 samples]\n",
      "Loss: 0.668429  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6906\n",
      "Test loss: 0.662320 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "Loss: 0.720292  [    0/ 8000 samples]\n",
      "Loss: 0.585296  [ 3200/ 8000 samples]\n",
      "Loss: 0.634048  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6713\n",
      "Test loss: 0.681945 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "Loss: 0.702841  [    0/ 8000 samples]\n",
      "Loss: 0.743861  [ 3200/ 8000 samples]\n",
      "Loss: 0.705718  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6988\n",
      "Test loss: 0.707837 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "Loss: 0.467171  [    0/ 8000 samples]\n",
      "Loss: 0.554711  [ 3200/ 8000 samples]\n",
      "Loss: 0.637494  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6964\n",
      "Test loss: 0.675024 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "Loss: 0.648699  [    0/ 8000 samples]\n",
      "Loss: 0.669449  [ 3200/ 8000 samples]\n",
      "Loss: 0.724650  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6810\n",
      "Test loss: 0.643204 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "Loss: 0.588787  [    0/ 8000 samples]\n",
      "Loss: 0.625740  [ 3200/ 8000 samples]\n",
      "Loss: 0.875943  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6705\n",
      "Test loss: 0.661138 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "Loss: 0.753783  [    0/ 8000 samples]\n",
      "Loss: 0.744690  [ 3200/ 8000 samples]\n",
      "Loss: 0.778249  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6660\n",
      "Test loss: 0.695943 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "Loss: 0.720381  [    0/ 8000 samples]\n",
      "Loss: 0.962905  [ 3200/ 8000 samples]\n",
      "Loss: 0.512092  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6822\n",
      "Test loss: 0.673413 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "Loss: 0.607579  [    0/ 8000 samples]\n",
      "Loss: 0.534100  [ 3200/ 8000 samples]\n",
      "Loss: 0.745977  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6581\n",
      "Test loss: 0.648246 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "Loss: 0.671037  [    0/ 8000 samples]\n",
      "Loss: 0.753870  [ 3200/ 8000 samples]\n",
      "Loss: 0.543759  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6684\n",
      "Test loss: 0.658032 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "Loss: 0.555588  [    0/ 8000 samples]\n",
      "Loss: 0.950211  [ 3200/ 8000 samples]\n",
      "Loss: 0.829310  [ 6400/ 8000 samples]\n",
      "Average loss: 0.7036\n",
      "Test loss: 0.646514 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "Loss: 0.750297  [    0/ 8000 samples]\n",
      "Loss: 0.593524  [ 3200/ 8000 samples]\n",
      "Loss: 0.681614  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6737\n",
      "Test loss: 0.648182 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "Loss: 0.824469  [    0/ 8000 samples]\n",
      "Loss: 0.457453  [ 3200/ 8000 samples]\n",
      "Loss: 0.424069  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6498\n",
      "Test loss: 0.610868 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "Loss: 0.534063  [    0/ 8000 samples]\n",
      "Loss: 0.671178  [ 3200/ 8000 samples]\n",
      "Loss: 0.733697  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6716\n",
      "Test loss: 0.738401 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "Loss: 0.778376  [    0/ 8000 samples]\n",
      "Loss: 0.547191  [ 3200/ 8000 samples]\n",
      "Loss: 0.590953  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6791\n",
      "Test loss: 0.661842 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "Loss: 0.683666  [    0/ 8000 samples]\n",
      "Loss: 0.902953  [ 3200/ 8000 samples]\n",
      "Loss: 0.899545  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6785\n",
      "Test loss: 0.755910 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "Loss: 0.641122  [    0/ 8000 samples]\n",
      "Loss: 0.516298  [ 3200/ 8000 samples]\n",
      "Loss: 0.710256  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6805\n",
      "Test loss: 0.731220 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "Loss: 0.884239  [    0/ 8000 samples]\n",
      "Loss: 0.673382  [ 3200/ 8000 samples]\n",
      "Loss: 0.853841  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6836\n",
      "Test loss: 0.679753 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "Loss: 0.632287  [    0/ 8000 samples]\n",
      "Loss: 0.491664  [ 3200/ 8000 samples]\n",
      "Loss: 0.530651  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6799\n",
      "Test loss: 0.660531 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "Loss: 0.736567  [    0/ 8000 samples]\n",
      "Loss: 0.836289  [ 3200/ 8000 samples]\n",
      "Loss: 0.523273  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6804\n",
      "Test loss: 0.714741 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "Loss: 1.154321  [    0/ 8000 samples]\n",
      "Loss: 0.403593  [ 3200/ 8000 samples]\n",
      "Loss: 0.480552  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6772\n",
      "Test loss: 0.692092 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "Loss: 0.914204  [    0/ 8000 samples]\n",
      "Loss: 0.506231  [ 3200/ 8000 samples]\n",
      "Loss: 0.581959  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6439\n",
      "Test loss: 0.653505 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "Loss: 0.571812  [    0/ 8000 samples]\n",
      "Loss: 0.631468  [ 3200/ 8000 samples]\n",
      "Loss: 0.510984  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6349\n",
      "Test loss: 0.648621 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "Loss: 0.568119  [    0/ 8000 samples]\n",
      "Loss: 0.652636  [ 3200/ 8000 samples]\n",
      "Loss: 0.516719  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6797\n",
      "Test loss: 0.667753 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "Loss: 0.594642  [    0/ 8000 samples]\n",
      "Loss: 0.683635  [ 3200/ 8000 samples]\n",
      "Loss: 0.549971  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6631\n",
      "Test loss: 0.654813 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "Loss: 0.565100  [    0/ 8000 samples]\n",
      "Loss: 0.664631  [ 3200/ 8000 samples]\n",
      "Loss: 0.599678  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6684\n",
      "Test loss: 0.632659 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "Loss: 0.648507  [    0/ 8000 samples]\n",
      "Loss: 0.641138  [ 3200/ 8000 samples]\n",
      "Loss: 0.788754  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6648\n",
      "Test loss: 0.706546 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "Loss: 0.697815  [    0/ 8000 samples]\n",
      "Loss: 0.598381  [ 3200/ 8000 samples]\n",
      "Loss: 0.592414  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6277\n",
      "Test loss: 0.711346 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "Loss: 0.696265  [    0/ 8000 samples]\n",
      "Loss: 0.677425  [ 3200/ 8000 samples]\n",
      "Loss: 0.548213  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6773\n",
      "Test loss: 0.708396 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "Loss: 0.475987  [    0/ 8000 samples]\n",
      "Loss: 0.512966  [ 3200/ 8000 samples]\n",
      "Loss: 0.801502  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6481\n",
      "Test loss: 0.696373 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "Loss: 1.025212  [    0/ 8000 samples]\n",
      "Loss: 0.536756  [ 3200/ 8000 samples]\n",
      "Loss: 0.709886  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6370\n",
      "Test loss: 0.636472 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "Loss: 0.866657  [    0/ 8000 samples]\n",
      "Loss: 0.483792  [ 3200/ 8000 samples]\n",
      "Loss: 0.509455  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6646\n",
      "Test loss: 0.657812 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "Loss: 0.562966  [    0/ 8000 samples]\n",
      "Loss: 0.471206  [ 3200/ 8000 samples]\n",
      "Loss: 0.641538  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6508\n",
      "Test loss: 0.621623 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "Loss: 0.786695  [    0/ 8000 samples]\n",
      "Loss: 0.525040  [ 3200/ 8000 samples]\n",
      "Loss: 0.542247  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6355\n",
      "Test loss: 0.679590 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "Loss: 0.507016  [    0/ 8000 samples]\n",
      "Loss: 0.543408  [ 3200/ 8000 samples]\n",
      "Loss: 0.511272  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6325\n",
      "Test loss: 0.672260 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "Loss: 0.850218  [    0/ 8000 samples]\n",
      "Loss: 0.573753  [ 3200/ 8000 samples]\n",
      "Loss: 0.775986  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6273\n",
      "Test loss: 0.650119 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "Loss: 0.628254  [    0/ 8000 samples]\n",
      "Loss: 0.470206  [ 3200/ 8000 samples]\n",
      "Loss: 0.595784  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6423\n",
      "Test loss: 0.664553 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "Loss: 0.757642  [    0/ 8000 samples]\n",
      "Loss: 0.606236  [ 3200/ 8000 samples]\n",
      "Loss: 0.621380  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6467\n",
      "Test loss: 0.663736 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "Loss: 0.571401  [    0/ 8000 samples]\n",
      "Loss: 0.603744  [ 3200/ 8000 samples]\n",
      "Loss: 0.595474  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6283\n",
      "Test loss: 0.667610 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "Loss: 0.828824  [    0/ 8000 samples]\n",
      "Loss: 0.678829  [ 3200/ 8000 samples]\n",
      "Loss: 0.527376  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6696\n",
      "Test loss: 0.633020 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Loss: 0.676074  [    0/ 8000 samples]\n",
      "Loss: 0.389141  [ 3200/ 8000 samples]\n",
      "Loss: 0.688666  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6249\n",
      "Test loss: 0.647866 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "Loss: 0.788629  [    0/ 8000 samples]\n",
      "Loss: 0.574276  [ 3200/ 8000 samples]\n",
      "Loss: 0.486779  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6388\n",
      "Test loss: 0.670433 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "Loss: 0.620727  [    0/ 8000 samples]\n",
      "Loss: 0.704278  [ 3200/ 8000 samples]\n",
      "Loss: 0.667841  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6667\n",
      "Test loss: 0.654938 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "Loss: 0.530039  [    0/ 8000 samples]\n",
      "Loss: 0.507357  [ 3200/ 8000 samples]\n",
      "Loss: 0.634828  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6315\n",
      "Test loss: 0.645814 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "Loss: 0.729489  [    0/ 8000 samples]\n",
      "Loss: 0.598224  [ 3200/ 8000 samples]\n",
      "Loss: 0.850242  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6493\n",
      "Test loss: 0.636692 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "Loss: 0.912746  [    0/ 8000 samples]\n",
      "Loss: 0.583236  [ 3200/ 8000 samples]\n",
      "Loss: 0.711507  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6600\n",
      "Test loss: 0.723915 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "Loss: 0.882313  [    0/ 8000 samples]\n",
      "Loss: 0.725189  [ 3200/ 8000 samples]\n",
      "Loss: 0.614151  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6577\n",
      "Test loss: 0.625578 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "Loss: 0.892464  [    0/ 8000 samples]\n",
      "Loss: 0.525132  [ 3200/ 8000 samples]\n",
      "Loss: 0.434469  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6515\n",
      "Test loss: 0.659698 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "Loss: 0.594057  [    0/ 8000 samples]\n",
      "Loss: 0.528657  [ 3200/ 8000 samples]\n",
      "Loss: 0.884732  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6722\n",
      "Test loss: 0.582276 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "Loss: 0.725097  [    0/ 8000 samples]\n",
      "Loss: 0.661801  [ 3200/ 8000 samples]\n",
      "Loss: 0.804181  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6335\n",
      "Test loss: 0.638670 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "Loss: 0.476058  [    0/ 8000 samples]\n",
      "Loss: 0.738539  [ 3200/ 8000 samples]\n",
      "Loss: 0.724083  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6414\n",
      "Test loss: 0.638755 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "Loss: 0.610610  [    0/ 8000 samples]\n",
      "Loss: 0.727305  [ 3200/ 8000 samples]\n",
      "Loss: 0.743991  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6483\n",
      "Test loss: 0.647472 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "Loss: 0.690953  [    0/ 8000 samples]\n",
      "Loss: 0.452175  [ 3200/ 8000 samples]\n",
      "Loss: 0.461137  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6317\n",
      "Test loss: 0.622940 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "Loss: 0.820128  [    0/ 8000 samples]\n",
      "Loss: 0.676501  [ 3200/ 8000 samples]\n",
      "Loss: 0.739423  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6398\n",
      "Test loss: 0.632690 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "Loss: 0.700943  [    0/ 8000 samples]\n",
      "Loss: 0.890355  [ 3200/ 8000 samples]\n",
      "Loss: 0.811394  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6423\n",
      "Test loss: 0.612113 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "Loss: 0.564732  [    0/ 8000 samples]\n",
      "Loss: 0.474739  [ 3200/ 8000 samples]\n",
      "Loss: 0.464720  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6639\n",
      "Test loss: 0.634564 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "Loss: 0.742166  [    0/ 8000 samples]\n",
      "Loss: 0.745907  [ 3200/ 8000 samples]\n",
      "Loss: 0.602041  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6295\n",
      "Test loss: 0.650553 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "Loss: 0.650661  [    0/ 8000 samples]\n",
      "Loss: 0.591622  [ 3200/ 8000 samples]\n",
      "Loss: 0.751438  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6248\n",
      "Test loss: 0.630154 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "Loss: 0.580519  [    0/ 8000 samples]\n",
      "Loss: 0.678096  [ 3200/ 8000 samples]\n",
      "Loss: 0.626358  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6512\n",
      "Test loss: 0.616462 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "Loss: 0.617187  [    0/ 8000 samples]\n",
      "Loss: 0.541927  [ 3200/ 8000 samples]\n",
      "Loss: 0.759782  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6238\n",
      "Test loss: 0.604341 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "Loss: 0.714451  [    0/ 8000 samples]\n",
      "Loss: 0.628384  [ 3200/ 8000 samples]\n",
      "Loss: 0.566902  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6279\n",
      "Test loss: 0.647402 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "Loss: 0.620792  [    0/ 8000 samples]\n",
      "Loss: 0.758605  [ 3200/ 8000 samples]\n",
      "Loss: 0.699381  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6231\n",
      "Test loss: 0.652851 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "Loss: 0.601123  [    0/ 8000 samples]\n",
      "Loss: 0.490970  [ 3200/ 8000 samples]\n",
      "Loss: 0.735369  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6375\n",
      "Test loss: 0.626920 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "Loss: 0.605296  [    0/ 8000 samples]\n",
      "Loss: 0.608200  [ 3200/ 8000 samples]\n",
      "Loss: 0.776597  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6350\n",
      "Test loss: 0.644099 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "Loss: 0.678430  [    0/ 8000 samples]\n",
      "Loss: 0.753622  [ 3200/ 8000 samples]\n",
      "Loss: 0.392267  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6287\n",
      "Test loss: 0.620513 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "Loss: 0.689156  [    0/ 8000 samples]\n",
      "Loss: 0.497254  [ 3200/ 8000 samples]\n",
      "Loss: 0.625585  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5996\n",
      "Test loss: 0.643915 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "Loss: 0.668976  [    0/ 8000 samples]\n",
      "Loss: 0.589020  [ 3200/ 8000 samples]\n",
      "Loss: 0.563640  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6513\n",
      "Test loss: 0.651057 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "Loss: 0.648179  [    0/ 8000 samples]\n",
      "Loss: 0.444722  [ 3200/ 8000 samples]\n",
      "Loss: 0.499591  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6093\n",
      "Test loss: 0.595843 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "Loss: 0.705862  [    0/ 8000 samples]\n",
      "Loss: 0.622881  [ 3200/ 8000 samples]\n",
      "Loss: 0.932567  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6192\n",
      "Test loss: 0.613544 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "Loss: 0.588473  [    0/ 8000 samples]\n",
      "Loss: 0.632832  [ 3200/ 8000 samples]\n",
      "Loss: 0.706160  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6325\n",
      "Test loss: 0.654690 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "Loss: 0.556865  [    0/ 8000 samples]\n",
      "Loss: 0.750595  [ 3200/ 8000 samples]\n",
      "Loss: 0.538004  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6387\n",
      "Test loss: 0.677444 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "Loss: 0.817732  [    0/ 8000 samples]\n",
      "Loss: 0.701436  [ 3200/ 8000 samples]\n",
      "Loss: 0.664538  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6219\n",
      "Test loss: 0.670454 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "Loss: 0.733962  [    0/ 8000 samples]\n",
      "Loss: 0.796401  [ 3200/ 8000 samples]\n",
      "Loss: 0.609055  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6416\n",
      "Test loss: 0.607117 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "Loss: 0.755874  [    0/ 8000 samples]\n",
      "Loss: 0.797517  [ 3200/ 8000 samples]\n",
      "Loss: 0.612191  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6296\n",
      "Test loss: 0.622620 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "Loss: 0.605213  [    0/ 8000 samples]\n",
      "Loss: 0.702321  [ 3200/ 8000 samples]\n",
      "Loss: 0.704348  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6533\n",
      "Test loss: 0.625470 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "Loss: 0.635795  [    0/ 8000 samples]\n",
      "Loss: 0.598577  [ 3200/ 8000 samples]\n",
      "Loss: 0.487281  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6229\n",
      "Test loss: 0.650704 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "Loss: 0.610759  [    0/ 8000 samples]\n",
      "Loss: 0.549573  [ 3200/ 8000 samples]\n",
      "Loss: 0.580250  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6327\n",
      "Test loss: 0.642602 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "Loss: 0.519561  [    0/ 8000 samples]\n",
      "Loss: 0.665423  [ 3200/ 8000 samples]\n",
      "Loss: 0.489292  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6233\n",
      "Test loss: 0.646033 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "Loss: 0.554657  [    0/ 8000 samples]\n",
      "Loss: 0.685621  [ 3200/ 8000 samples]\n",
      "Loss: 0.476350  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6097\n",
      "Test loss: 0.596028 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "Loss: 0.746147  [    0/ 8000 samples]\n",
      "Loss: 0.746661  [ 3200/ 8000 samples]\n",
      "Loss: 0.655059  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6337\n",
      "Test loss: 0.714265 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "Loss: 0.714784  [    0/ 8000 samples]\n",
      "Loss: 0.426867  [ 3200/ 8000 samples]\n",
      "Loss: 0.800178  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6186\n",
      "Test loss: 0.763960 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "Loss: 0.792797  [    0/ 8000 samples]\n",
      "Loss: 0.543640  [ 3200/ 8000 samples]\n",
      "Loss: 0.690645  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6210\n",
      "Test loss: 0.695207 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "Loss: 0.663302  [    0/ 8000 samples]\n",
      "Loss: 0.532636  [ 3200/ 8000 samples]\n",
      "Loss: 0.599578  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6051\n",
      "Test loss: 0.606220 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "Loss: 0.469228  [    0/ 8000 samples]\n",
      "Loss: 0.554806  [ 3200/ 8000 samples]\n",
      "Loss: 0.807421  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6583\n",
      "Test loss: 0.626429 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "Loss: 0.583266  [    0/ 8000 samples]\n",
      "Loss: 0.558930  [ 3200/ 8000 samples]\n",
      "Loss: 0.639656  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5965\n",
      "Test loss: 0.585970 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "Loss: 0.641844  [    0/ 8000 samples]\n",
      "Loss: 0.942562  [ 3200/ 8000 samples]\n",
      "Loss: 0.714306  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5996\n",
      "Test loss: 0.642657 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "Loss: 0.583171  [    0/ 8000 samples]\n",
      "Loss: 0.465324  [ 3200/ 8000 samples]\n",
      "Loss: 0.639245  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6148\n",
      "Test loss: 0.571232 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "Loss: 0.414068  [    0/ 8000 samples]\n",
      "Loss: 0.570033  [ 3200/ 8000 samples]\n",
      "Loss: 0.639988  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6174\n",
      "Test loss: 0.607909 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "Loss: 0.716524  [    0/ 8000 samples]\n",
      "Loss: 0.853098  [ 3200/ 8000 samples]\n",
      "Loss: 0.591949  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6097\n",
      "Test loss: 0.597012 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "Loss: 0.597395  [    0/ 8000 samples]\n",
      "Loss: 0.690357  [ 3200/ 8000 samples]\n",
      "Loss: 0.739004  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5934\n",
      "Test loss: 0.669246 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Loss: 0.478146  [    0/ 8000 samples]\n",
      "Loss: 0.736188  [ 3200/ 8000 samples]\n",
      "Loss: 0.809240  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5971\n",
      "Test loss: 0.648842 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "Loss: 0.572389  [    0/ 8000 samples]\n",
      "Loss: 0.628715  [ 3200/ 8000 samples]\n",
      "Loss: 0.679033  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6264\n",
      "Test loss: 0.587445 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "Loss: 0.615485  [    0/ 8000 samples]\n",
      "Loss: 0.633523  [ 3200/ 8000 samples]\n",
      "Loss: 0.968099  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6347\n",
      "Test loss: 0.628259 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "Loss: 0.600101  [    0/ 8000 samples]\n",
      "Loss: 0.627745  [ 3200/ 8000 samples]\n",
      "Loss: 0.405981  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6144\n",
      "Test loss: 0.630448 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "Loss: 0.713213  [    0/ 8000 samples]\n",
      "Loss: 0.584732  [ 3200/ 8000 samples]\n",
      "Loss: 0.394095  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6069\n",
      "Test loss: 0.632270 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "Loss: 0.561415  [    0/ 8000 samples]\n",
      "Loss: 0.694313  [ 3200/ 8000 samples]\n",
      "Loss: 0.629792  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6266\n",
      "Test loss: 0.631601 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "Loss: 0.586332  [    0/ 8000 samples]\n",
      "Loss: 0.593839  [ 3200/ 8000 samples]\n",
      "Loss: 0.524580  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6091\n",
      "Test loss: 0.614507 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "Loss: 0.559287  [    0/ 8000 samples]\n",
      "Loss: 0.640000  [ 3200/ 8000 samples]\n",
      "Loss: 0.850099  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6066\n",
      "Test loss: 0.578439 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "Loss: 0.712564  [    0/ 8000 samples]\n",
      "Loss: 0.616614  [ 3200/ 8000 samples]\n",
      "Loss: 0.620668  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6007\n",
      "Test loss: 0.656752 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "Loss: 0.668066  [    0/ 8000 samples]\n",
      "Loss: 0.523783  [ 3200/ 8000 samples]\n",
      "Loss: 0.784914  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6299\n",
      "Test loss: 0.605223 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "Loss: 0.798768  [    0/ 8000 samples]\n",
      "Loss: 0.725434  [ 3200/ 8000 samples]\n",
      "Loss: 0.673408  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6302\n",
      "Test loss: 0.622507 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "Loss: 0.571112  [    0/ 8000 samples]\n",
      "Loss: 0.732180  [ 3200/ 8000 samples]\n",
      "Loss: 0.945480  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5885\n",
      "Test loss: 0.637109 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "Loss: 0.647161  [    0/ 8000 samples]\n",
      "Loss: 0.865557  [ 3200/ 8000 samples]\n",
      "Loss: 0.520963  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6066\n",
      "Test loss: 0.605226 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "Loss: 0.439439  [    0/ 8000 samples]\n",
      "Loss: 0.599510  [ 3200/ 8000 samples]\n",
      "Loss: 0.584701  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6021\n",
      "Test loss: 0.573282 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "Loss: 0.409367  [    0/ 8000 samples]\n",
      "Loss: 0.564110  [ 3200/ 8000 samples]\n",
      "Loss: 0.647936  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6420\n",
      "Test loss: 0.603584 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "Loss: 0.869228  [    0/ 8000 samples]\n",
      "Loss: 0.608402  [ 3200/ 8000 samples]\n",
      "Loss: 0.549517  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6080\n",
      "Test loss: 0.587399 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "Loss: 0.747073  [    0/ 8000 samples]\n",
      "Loss: 0.393089  [ 3200/ 8000 samples]\n",
      "Loss: 0.482638  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6015\n",
      "Test loss: 0.557509 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "Loss: 0.412467  [    0/ 8000 samples]\n",
      "Loss: 0.637672  [ 3200/ 8000 samples]\n",
      "Loss: 0.459000  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6191\n",
      "Test loss: 0.660910 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "Loss: 0.743508  [    0/ 8000 samples]\n",
      "Loss: 0.642865  [ 3200/ 8000 samples]\n",
      "Loss: 0.501031  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5942\n",
      "Test loss: 0.649239 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "Loss: 0.604410  [    0/ 8000 samples]\n",
      "Loss: 0.563339  [ 3200/ 8000 samples]\n",
      "Loss: 0.572307  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5929\n",
      "Test loss: 0.604867 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "Loss: 0.518901  [    0/ 8000 samples]\n",
      "Loss: 0.483135  [ 3200/ 8000 samples]\n",
      "Loss: 0.649444  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6024\n",
      "Test loss: 0.573739 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "Loss: 0.804287  [    0/ 8000 samples]\n",
      "Loss: 0.528418  [ 3200/ 8000 samples]\n",
      "Loss: 0.447949  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6266\n",
      "Test loss: 0.592053 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "Loss: 0.567598  [    0/ 8000 samples]\n",
      "Loss: 0.689099  [ 3200/ 8000 samples]\n",
      "Loss: 0.684197  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5914\n",
      "Test loss: 0.633276 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "Loss: 0.601654  [    0/ 8000 samples]\n",
      "Loss: 0.439050  [ 3200/ 8000 samples]\n",
      "Loss: 0.592694  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5875\n",
      "Test loss: 0.578677 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "Loss: 0.483910  [    0/ 8000 samples]\n",
      "Loss: 0.629613  [ 3200/ 8000 samples]\n",
      "Loss: 0.749711  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6121\n",
      "Test loss: 0.572271 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "Loss: 0.511478  [    0/ 8000 samples]\n",
      "Loss: 0.574947  [ 3200/ 8000 samples]\n",
      "Loss: 0.682594  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5972\n",
      "Test loss: 0.636022 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "Loss: 0.438198  [    0/ 8000 samples]\n",
      "Loss: 0.454054  [ 3200/ 8000 samples]\n",
      "Loss: 0.720580  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6022\n",
      "Test loss: 0.589318 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "Loss: 0.522718  [    0/ 8000 samples]\n",
      "Loss: 0.476485  [ 3200/ 8000 samples]\n",
      "Loss: 0.547809  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6068\n",
      "Test loss: 0.630054 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "Loss: 0.601960  [    0/ 8000 samples]\n",
      "Loss: 0.720965  [ 3200/ 8000 samples]\n",
      "Loss: 0.628414  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5808\n",
      "Test loss: 0.586949 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "Loss: 0.789238  [    0/ 8000 samples]\n",
      "Loss: 0.438491  [ 3200/ 8000 samples]\n",
      "Loss: 0.470144  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5878\n",
      "Test loss: 0.566382 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "Loss: 0.524628  [    0/ 8000 samples]\n",
      "Loss: 0.535419  [ 3200/ 8000 samples]\n",
      "Loss: 0.721603  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6021\n",
      "Test loss: 0.628371 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "Loss: 0.431801  [    0/ 8000 samples]\n",
      "Loss: 0.760397  [ 3200/ 8000 samples]\n",
      "Loss: 0.409903  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6032\n",
      "Test loss: 0.625676 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "Loss: 0.599260  [    0/ 8000 samples]\n",
      "Loss: 0.647482  [ 3200/ 8000 samples]\n",
      "Loss: 0.669989  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6055\n",
      "Test loss: 0.642221 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "Loss: 0.827469  [    0/ 8000 samples]\n",
      "Loss: 0.586998  [ 3200/ 8000 samples]\n",
      "Loss: 0.630159  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6107\n",
      "Test loss: 0.700941 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "Loss: 0.677498  [    0/ 8000 samples]\n",
      "Loss: 0.533697  [ 3200/ 8000 samples]\n",
      "Loss: 0.636057  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5889\n",
      "Test loss: 0.662222 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "Loss: 0.646154  [    0/ 8000 samples]\n",
      "Loss: 0.553231  [ 3200/ 8000 samples]\n",
      "Loss: 0.552870  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6000\n",
      "Test loss: 0.576146 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "Loss: 0.706486  [    0/ 8000 samples]\n",
      "Loss: 0.481736  [ 3200/ 8000 samples]\n",
      "Loss: 0.620207  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5819\n",
      "Test loss: 0.571496 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "Loss: 0.609375  [    0/ 8000 samples]\n",
      "Loss: 0.595463  [ 3200/ 8000 samples]\n",
      "Loss: 0.374127  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6012\n",
      "Test loss: 0.641438 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "Loss: 0.452977  [    0/ 8000 samples]\n",
      "Loss: 0.537761  [ 3200/ 8000 samples]\n",
      "Loss: 0.737003  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5893\n",
      "Test loss: 0.618268 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "Loss: 0.812565  [    0/ 8000 samples]\n",
      "Loss: 0.356344  [ 3200/ 8000 samples]\n",
      "Loss: 0.630137  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5906\n",
      "Test loss: 0.635390 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "Loss: 0.667580  [    0/ 8000 samples]\n",
      "Loss: 0.347149  [ 3200/ 8000 samples]\n",
      "Loss: 0.502411  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5717\n",
      "Test loss: 0.589559 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "Loss: 0.569736  [    0/ 8000 samples]\n",
      "Loss: 0.552323  [ 3200/ 8000 samples]\n",
      "Loss: 1.003537  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5897\n",
      "Test loss: 0.601566 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "Loss: 0.491326  [    0/ 8000 samples]\n",
      "Loss: 0.537484  [ 3200/ 8000 samples]\n",
      "Loss: 0.603286  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5847\n",
      "Test loss: 0.570651 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "Loss: 0.443393  [    0/ 8000 samples]\n",
      "Loss: 0.551856  [ 3200/ 8000 samples]\n",
      "Loss: 0.750635  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5973\n",
      "Test loss: 0.568547 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "Loss: 0.537678  [    0/ 8000 samples]\n",
      "Loss: 0.538170  [ 3200/ 8000 samples]\n",
      "Loss: 0.543393  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6070\n",
      "Test loss: 0.584767 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "Loss: 0.536692  [    0/ 8000 samples]\n",
      "Loss: 0.656928  [ 3200/ 8000 samples]\n",
      "Loss: 0.675462  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5892\n",
      "Test loss: 0.612815 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "Loss: 0.575392  [    0/ 8000 samples]\n",
      "Loss: 0.755188  [ 3200/ 8000 samples]\n",
      "Loss: 0.555940  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5824\n",
      "Test loss: 0.532582 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "Loss: 0.518243  [    0/ 8000 samples]\n",
      "Loss: 0.659432  [ 3200/ 8000 samples]\n",
      "Loss: 0.626578  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5977\n",
      "Test loss: 0.588782 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "Loss: 0.476168  [    0/ 8000 samples]\n",
      "Loss: 0.544408  [ 3200/ 8000 samples]\n",
      "Loss: 0.491664  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5811\n",
      "Test loss: 0.594945 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "Loss: 0.651675  [    0/ 8000 samples]\n",
      "Loss: 0.597908  [ 3200/ 8000 samples]\n",
      "Loss: 0.494064  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5979\n",
      "Test loss: 0.569026 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Loss: 0.684931  [    0/ 8000 samples]\n",
      "Loss: 0.376379  [ 3200/ 8000 samples]\n",
      "Loss: 0.647367  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5613\n",
      "Test loss: 0.601581 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "Loss: 0.616144  [    0/ 8000 samples]\n",
      "Loss: 0.545720  [ 3200/ 8000 samples]\n",
      "Loss: 0.534749  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5785\n",
      "Test loss: 0.623252 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "Loss: 0.479922  [    0/ 8000 samples]\n",
      "Loss: 0.591198  [ 3200/ 8000 samples]\n",
      "Loss: 0.684915  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5931\n",
      "Test loss: 0.565898 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "Loss: 0.740250  [    0/ 8000 samples]\n",
      "Loss: 0.713983  [ 3200/ 8000 samples]\n",
      "Loss: 0.624283  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5953\n",
      "Test loss: 0.596140 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "Loss: 0.570618  [    0/ 8000 samples]\n",
      "Loss: 0.652079  [ 3200/ 8000 samples]\n",
      "Loss: 0.517152  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5788\n",
      "Test loss: 0.604662 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "Loss: 0.481364  [    0/ 8000 samples]\n",
      "Loss: 0.661847  [ 3200/ 8000 samples]\n",
      "Loss: 0.444626  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5906\n",
      "Test loss: 0.569321 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "Loss: 0.394431  [    0/ 8000 samples]\n",
      "Loss: 0.585154  [ 3200/ 8000 samples]\n",
      "Loss: 0.407388  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5819\n",
      "Test loss: 0.606623 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "Loss: 0.639989  [    0/ 8000 samples]\n",
      "Loss: 0.607446  [ 3200/ 8000 samples]\n",
      "Loss: 0.549849  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5972\n",
      "Test loss: 0.677881 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "Loss: 0.484354  [    0/ 8000 samples]\n",
      "Loss: 0.416456  [ 3200/ 8000 samples]\n",
      "Loss: 0.736065  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6080\n",
      "Test loss: 0.605463 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "Loss: 0.840831  [    0/ 8000 samples]\n",
      "Loss: 0.690254  [ 3200/ 8000 samples]\n",
      "Loss: 0.782220  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5905\n",
      "Test loss: 0.556744 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "Loss: 0.526339  [    0/ 8000 samples]\n",
      "Loss: 0.524987  [ 3200/ 8000 samples]\n",
      "Loss: 0.624341  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5716\n",
      "Test loss: 0.586754 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "Loss: 0.807243  [    0/ 8000 samples]\n",
      "Loss: 0.710177  [ 3200/ 8000 samples]\n",
      "Loss: 0.533565  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5920\n",
      "Test loss: 0.556175 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "Loss: 0.485002  [    0/ 8000 samples]\n",
      "Loss: 0.706145  [ 3200/ 8000 samples]\n",
      "Loss: 0.626800  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5774\n",
      "Test loss: 0.548020 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "Loss: 0.444716  [    0/ 8000 samples]\n",
      "Loss: 0.600654  [ 3200/ 8000 samples]\n",
      "Loss: 0.474665  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5893\n",
      "Test loss: 0.588720 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "Loss: 0.395508  [    0/ 8000 samples]\n",
      "Loss: 0.508121  [ 3200/ 8000 samples]\n",
      "Loss: 0.752550  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6019\n",
      "Test loss: 0.581563 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "Loss: 0.402717  [    0/ 8000 samples]\n",
      "Loss: 0.307846  [ 3200/ 8000 samples]\n",
      "Loss: 0.435318  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5819\n",
      "Test loss: 0.581889 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "Loss: 0.555767  [    0/ 8000 samples]\n",
      "Loss: 0.600146  [ 3200/ 8000 samples]\n",
      "Loss: 0.563859  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5797\n",
      "Test loss: 0.644575 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "Loss: 0.722111  [    0/ 8000 samples]\n",
      "Loss: 0.586460  [ 3200/ 8000 samples]\n",
      "Loss: 0.455389  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5703\n",
      "Test loss: 0.607590 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "Loss: 0.517665  [    0/ 8000 samples]\n",
      "Loss: 0.457824  [ 3200/ 8000 samples]\n",
      "Loss: 0.550024  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5901\n",
      "Test loss: 0.610054 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "Loss: 0.614512  [    0/ 8000 samples]\n",
      "Loss: 0.520016  [ 3200/ 8000 samples]\n",
      "Loss: 0.532582  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5976\n",
      "Test loss: 0.594941 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "Loss: 0.443090  [    0/ 8000 samples]\n",
      "Loss: 0.702751  [ 3200/ 8000 samples]\n",
      "Loss: 0.483047  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5895\n",
      "Test loss: 0.569479 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "Loss: 0.565065  [    0/ 8000 samples]\n",
      "Loss: 0.686213  [ 3200/ 8000 samples]\n",
      "Loss: 0.535546  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5815\n",
      "Test loss: 0.592242 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "Loss: 0.787412  [    0/ 8000 samples]\n",
      "Loss: 0.493440  [ 3200/ 8000 samples]\n",
      "Loss: 0.501417  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5855\n",
      "Test loss: 0.606687 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "Loss: 0.485003  [    0/ 8000 samples]\n",
      "Loss: 0.580499  [ 3200/ 8000 samples]\n",
      "Loss: 0.779805  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5648\n",
      "Test loss: 0.557120 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "Loss: 0.465165  [    0/ 8000 samples]\n",
      "Loss: 0.607304  [ 3200/ 8000 samples]\n",
      "Loss: 0.670106  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5655\n",
      "Test loss: 0.631472 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "Loss: 0.575349  [    0/ 8000 samples]\n",
      "Loss: 0.508719  [ 3200/ 8000 samples]\n",
      "Loss: 0.495415  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6033\n",
      "Test loss: 0.585312 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "Loss: 0.503628  [    0/ 8000 samples]\n",
      "Loss: 0.635870  [ 3200/ 8000 samples]\n",
      "Loss: 0.644468  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5453\n",
      "Test loss: 0.600170 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "Loss: 0.596870  [    0/ 8000 samples]\n",
      "Loss: 0.601723  [ 3200/ 8000 samples]\n",
      "Loss: 0.672076  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5782\n",
      "Test loss: 0.621633 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "Loss: 0.691947  [    0/ 8000 samples]\n",
      "Loss: 0.535995  [ 3200/ 8000 samples]\n",
      "Loss: 0.607816  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5777\n",
      "Test loss: 0.621865 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "Loss: 0.506023  [    0/ 8000 samples]\n",
      "Loss: 0.496611  [ 3200/ 8000 samples]\n",
      "Loss: 0.750128  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6037\n",
      "Test loss: 0.618600 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "Loss: 0.680557  [    0/ 8000 samples]\n",
      "Loss: 0.647259  [ 3200/ 8000 samples]\n",
      "Loss: 0.600254  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5832\n",
      "Test loss: 0.585970 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "Loss: 0.663170  [    0/ 8000 samples]\n",
      "Loss: 0.593470  [ 3200/ 8000 samples]\n",
      "Loss: 0.581598  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5826\n",
      "Test loss: 0.563590 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "Loss: 0.635239  [    0/ 8000 samples]\n",
      "Loss: 0.431037  [ 3200/ 8000 samples]\n",
      "Loss: 0.742841  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5689\n",
      "Test loss: 0.582480 \n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "Loss: 0.660836  [    0/ 8000 samples]\n",
      "Loss: 0.504931  [ 3200/ 8000 samples]\n",
      "Loss: 0.470680  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5627\n",
      "Test loss: 0.569843 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "Loss: 0.526526  [    0/ 8000 samples]\n",
      "Loss: 0.327198  [ 3200/ 8000 samples]\n",
      "Loss: 0.619638  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5782\n",
      "Test loss: 0.608917 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "Loss: 0.462033  [    0/ 8000 samples]\n",
      "Loss: 0.407419  [ 3200/ 8000 samples]\n",
      "Loss: 0.615399  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5569\n",
      "Test loss: 0.566488 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "Loss: 0.551521  [    0/ 8000 samples]\n",
      "Loss: 0.806255  [ 3200/ 8000 samples]\n",
      "Loss: 0.473649  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5565\n",
      "Test loss: 0.538306 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "Loss: 0.613096  [    0/ 8000 samples]\n",
      "Loss: 0.535730  [ 3200/ 8000 samples]\n",
      "Loss: 0.478583  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5759\n",
      "Test loss: 0.544301 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "Loss: 0.406384  [    0/ 8000 samples]\n",
      "Loss: 0.672394  [ 3200/ 8000 samples]\n",
      "Loss: 0.688356  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5688\n",
      "Test loss: 0.579386 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "Loss: 0.670696  [    0/ 8000 samples]\n",
      "Loss: 0.527110  [ 3200/ 8000 samples]\n",
      "Loss: 0.584329  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5994\n",
      "Test loss: 0.595854 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "Loss: 0.355476  [    0/ 8000 samples]\n",
      "Loss: 0.459652  [ 3200/ 8000 samples]\n",
      "Loss: 0.339440  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5689\n",
      "Test loss: 0.609364 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "Loss: 0.564864  [    0/ 8000 samples]\n",
      "Loss: 0.604099  [ 3200/ 8000 samples]\n",
      "Loss: 0.508864  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5539\n",
      "Test loss: 0.582453 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "Loss: 0.738819  [    0/ 8000 samples]\n",
      "Loss: 0.715369  [ 3200/ 8000 samples]\n",
      "Loss: 0.429140  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5904\n",
      "Test loss: 0.551751 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "Loss: 0.577381  [    0/ 8000 samples]\n",
      "Loss: 0.669847  [ 3200/ 8000 samples]\n",
      "Loss: 0.436904  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5683\n",
      "Test loss: 0.615893 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "Loss: 0.581235  [    0/ 8000 samples]\n",
      "Loss: 0.603992  [ 3200/ 8000 samples]\n",
      "Loss: 0.560351  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5929\n",
      "Test loss: 0.615425 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "Loss: 0.732460  [    0/ 8000 samples]\n",
      "Loss: 0.939277  [ 3200/ 8000 samples]\n",
      "Loss: 0.427311  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5765\n",
      "Test loss: 0.598159 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "Loss: 0.688327  [    0/ 8000 samples]\n",
      "Loss: 0.499668  [ 3200/ 8000 samples]\n",
      "Loss: 0.502965  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5727\n",
      "Test loss: 0.598380 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "Loss: 0.541469  [    0/ 8000 samples]\n",
      "Loss: 0.510058  [ 3200/ 8000 samples]\n",
      "Loss: 0.380651  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5743\n",
      "Test loss: 0.578515 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "Loss: 0.467576  [    0/ 8000 samples]\n",
      "Loss: 0.799361  [ 3200/ 8000 samples]\n",
      "Loss: 0.535741  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5573\n",
      "Test loss: 0.570404 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "Loss: 0.486563  [    0/ 8000 samples]\n",
      "Loss: 0.481624  [ 3200/ 8000 samples]\n",
      "Loss: 0.575970  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5525\n",
      "Test loss: 0.641175 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Loss: 0.585499  [    0/ 8000 samples]\n",
      "Loss: 0.576190  [ 3200/ 8000 samples]\n",
      "Loss: 0.575780  [ 6400/ 8000 samples]\n",
      "Average loss: 0.6082\n",
      "Test loss: 0.530708 \n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "Loss: 0.551768  [    0/ 8000 samples]\n",
      "Loss: 0.547003  [ 3200/ 8000 samples]\n",
      "Loss: 0.744278  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5681\n",
      "Test loss: 0.563080 \n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "Loss: 0.591753  [    0/ 8000 samples]\n",
      "Loss: 0.450004  [ 3200/ 8000 samples]\n",
      "Loss: 0.488416  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5669\n",
      "Test loss: 0.625325 \n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "Loss: 0.500953  [    0/ 8000 samples]\n",
      "Loss: 0.984076  [ 3200/ 8000 samples]\n",
      "Loss: 0.529108  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5762\n",
      "Test loss: 0.601428 \n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "Loss: 0.506747  [    0/ 8000 samples]\n",
      "Loss: 0.838199  [ 3200/ 8000 samples]\n",
      "Loss: 0.593160  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5609\n",
      "Test loss: 0.578844 \n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "Loss: 0.692420  [    0/ 8000 samples]\n",
      "Loss: 0.570701  [ 3200/ 8000 samples]\n",
      "Loss: 0.795878  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5534\n",
      "Test loss: 0.553154 \n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "Loss: 0.386806  [    0/ 8000 samples]\n",
      "Loss: 0.516160  [ 3200/ 8000 samples]\n",
      "Loss: 0.525608  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5533\n",
      "Test loss: 0.530769 \n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "Loss: 0.645429  [    0/ 8000 samples]\n",
      "Loss: 0.540611  [ 3200/ 8000 samples]\n",
      "Loss: 0.602064  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5457\n",
      "Test loss: 0.579567 \n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "Loss: 0.471198  [    0/ 8000 samples]\n",
      "Loss: 0.405520  [ 3200/ 8000 samples]\n",
      "Loss: 0.508352  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5595\n",
      "Test loss: 0.539542 \n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "Loss: 0.593022  [    0/ 8000 samples]\n",
      "Loss: 0.422573  [ 3200/ 8000 samples]\n",
      "Loss: 0.667215  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5777\n",
      "Test loss: 0.572195 \n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "Loss: 0.646721  [    0/ 8000 samples]\n",
      "Loss: 0.483220  [ 3200/ 8000 samples]\n",
      "Loss: 0.332562  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5457\n",
      "Test loss: 0.548364 \n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "Loss: 0.397462  [    0/ 8000 samples]\n",
      "Loss: 0.622516  [ 3200/ 8000 samples]\n",
      "Loss: 0.535315  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5664\n",
      "Test loss: 0.542176 \n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "Loss: 0.526433  [    0/ 8000 samples]\n",
      "Loss: 0.426986  [ 3200/ 8000 samples]\n",
      "Loss: 0.509134  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5476\n",
      "Test loss: 0.550736 \n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "Loss: 0.503480  [    0/ 8000 samples]\n",
      "Loss: 0.253862  [ 3200/ 8000 samples]\n",
      "Loss: 0.610835  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5505\n",
      "Test loss: 0.577119 \n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "Loss: 0.503123  [    0/ 8000 samples]\n",
      "Loss: 0.572999  [ 3200/ 8000 samples]\n",
      "Loss: 0.552732  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5594\n",
      "Test loss: 0.533474 \n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "Loss: 0.713000  [    0/ 8000 samples]\n",
      "Loss: 0.638199  [ 3200/ 8000 samples]\n",
      "Loss: 0.639086  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5658\n",
      "Test loss: 0.567320 \n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "Loss: 0.697814  [    0/ 8000 samples]\n",
      "Loss: 0.560245  [ 3200/ 8000 samples]\n",
      "Loss: 0.457797  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5775\n",
      "Test loss: 0.557728 \n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "Loss: 1.030175  [    0/ 8000 samples]\n",
      "Loss: 0.532618  [ 3200/ 8000 samples]\n",
      "Loss: 0.748549  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5773\n",
      "Test loss: 0.613373 \n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "Loss: 0.461350  [    0/ 8000 samples]\n",
      "Loss: 0.628851  [ 3200/ 8000 samples]\n",
      "Loss: 0.484074  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5676\n",
      "Test loss: 0.604703 \n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "Loss: 0.676775  [    0/ 8000 samples]\n",
      "Loss: 0.281136  [ 3200/ 8000 samples]\n",
      "Loss: 0.485100  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5824\n",
      "Test loss: 0.526725 \n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "Loss: 0.517432  [    0/ 8000 samples]\n",
      "Loss: 0.526265  [ 3200/ 8000 samples]\n",
      "Loss: 0.646332  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5541\n",
      "Test loss: 0.538957 \n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "Loss: 0.841380  [    0/ 8000 samples]\n",
      "Loss: 0.480869  [ 3200/ 8000 samples]\n",
      "Loss: 0.504137  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5661\n",
      "Test loss: 0.599819 \n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "Loss: 0.594718  [    0/ 8000 samples]\n",
      "Loss: 0.405098  [ 3200/ 8000 samples]\n",
      "Loss: 0.579578  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5687\n",
      "Test loss: 0.568588 \n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "Loss: 0.690162  [    0/ 8000 samples]\n",
      "Loss: 0.533602  [ 3200/ 8000 samples]\n",
      "Loss: 0.704034  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5615\n",
      "Test loss: 0.545815 \n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "Loss: 0.567967  [    0/ 8000 samples]\n",
      "Loss: 0.435898  [ 3200/ 8000 samples]\n",
      "Loss: 0.604238  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5451\n",
      "Test loss: 0.554432 \n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "Loss: 0.630210  [    0/ 8000 samples]\n",
      "Loss: 0.628279  [ 3200/ 8000 samples]\n",
      "Loss: 0.476594  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5515\n",
      "Test loss: 0.577645 \n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "Loss: 0.520647  [    0/ 8000 samples]\n",
      "Loss: 0.485551  [ 3200/ 8000 samples]\n",
      "Loss: 0.479331  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5554\n",
      "Test loss: 0.539299 \n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "Loss: 0.490572  [    0/ 8000 samples]\n",
      "Loss: 0.547400  [ 3200/ 8000 samples]\n",
      "Loss: 0.467885  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5602\n",
      "Test loss: 0.563087 \n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "Loss: 0.476532  [    0/ 8000 samples]\n",
      "Loss: 0.499248  [ 3200/ 8000 samples]\n",
      "Loss: 0.449579  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5597\n",
      "Test loss: 0.572466 \n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "Loss: 0.617608  [    0/ 8000 samples]\n",
      "Loss: 0.644678  [ 3200/ 8000 samples]\n",
      "Loss: 0.501495  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5795\n",
      "Test loss: 0.567907 \n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "Loss: 0.775135  [    0/ 8000 samples]\n",
      "Loss: 0.776960  [ 3200/ 8000 samples]\n",
      "Loss: 0.515727  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5875\n",
      "Test loss: 0.552144 \n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "Loss: 0.671880  [    0/ 8000 samples]\n",
      "Loss: 0.499431  [ 3200/ 8000 samples]\n",
      "Loss: 0.777063  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5830\n",
      "Test loss: 0.565852 \n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "Loss: 0.502725  [    0/ 8000 samples]\n",
      "Loss: 0.533452  [ 3200/ 8000 samples]\n",
      "Loss: 0.579492  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5417\n",
      "Test loss: 0.539738 \n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "Loss: 0.980270  [    0/ 8000 samples]\n",
      "Loss: 0.475529  [ 3200/ 8000 samples]\n",
      "Loss: 0.654479  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5359\n",
      "Test loss: 0.541992 \n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "Loss: 0.664849  [    0/ 8000 samples]\n",
      "Loss: 0.536551  [ 3200/ 8000 samples]\n",
      "Loss: 0.347133  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5680\n",
      "Test loss: 0.537629 \n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "Loss: 0.523070  [    0/ 8000 samples]\n",
      "Loss: 0.625638  [ 3200/ 8000 samples]\n",
      "Loss: 0.803639  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5671\n",
      "Test loss: 0.506800 \n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "Loss: 0.499451  [    0/ 8000 samples]\n",
      "Loss: 0.819154  [ 3200/ 8000 samples]\n",
      "Loss: 0.654210  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5545\n",
      "Test loss: 0.550195 \n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "Loss: 0.511729  [    0/ 8000 samples]\n",
      "Loss: 0.588990  [ 3200/ 8000 samples]\n",
      "Loss: 0.451332  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5791\n",
      "Test loss: 0.537173 \n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "Loss: 0.571066  [    0/ 8000 samples]\n",
      "Loss: 0.628017  [ 3200/ 8000 samples]\n",
      "Loss: 0.694151  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5669\n",
      "Test loss: 0.566547 \n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "Loss: 0.544828  [    0/ 8000 samples]\n",
      "Loss: 0.450855  [ 3200/ 8000 samples]\n",
      "Loss: 0.495312  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5374\n",
      "Test loss: 0.620887 \n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "Loss: 0.523192  [    0/ 8000 samples]\n",
      "Loss: 0.395805  [ 3200/ 8000 samples]\n",
      "Loss: 0.631449  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5542\n",
      "Test loss: 0.620751 \n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "Loss: 0.698073  [    0/ 8000 samples]\n",
      "Loss: 0.548399  [ 3200/ 8000 samples]\n",
      "Loss: 0.556977  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5745\n",
      "Test loss: 0.580132 \n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "Loss: 0.657462  [    0/ 8000 samples]\n",
      "Loss: 0.635343  [ 3200/ 8000 samples]\n",
      "Loss: 0.599959  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5632\n",
      "Test loss: 0.567343 \n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "Loss: 0.515977  [    0/ 8000 samples]\n",
      "Loss: 0.555394  [ 3200/ 8000 samples]\n",
      "Loss: 0.559993  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5425\n",
      "Test loss: 0.566105 \n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "Loss: 0.624936  [    0/ 8000 samples]\n",
      "Loss: 0.544905  [ 3200/ 8000 samples]\n",
      "Loss: 0.464000  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5720\n",
      "Test loss: 0.640208 \n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "Loss: 0.671679  [    0/ 8000 samples]\n",
      "Loss: 0.615093  [ 3200/ 8000 samples]\n",
      "Loss: 0.705697  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5846\n",
      "Test loss: 0.530858 \n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "Loss: 0.549402  [    0/ 8000 samples]\n",
      "Loss: 0.751089  [ 3200/ 8000 samples]\n",
      "Loss: 0.791337  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5706\n",
      "Test loss: 0.513226 \n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "Loss: 0.568301  [    0/ 8000 samples]\n",
      "Loss: 0.576295  [ 3200/ 8000 samples]\n",
      "Loss: 0.570461  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5555\n",
      "Test loss: 0.533669 \n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "Loss: 0.494352  [    0/ 8000 samples]\n",
      "Loss: 0.402812  [ 3200/ 8000 samples]\n",
      "Loss: 0.596459  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5378\n",
      "Test loss: 0.542855 \n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "Loss: 0.545989  [    0/ 8000 samples]\n",
      "Loss: 0.720034  [ 3200/ 8000 samples]\n",
      "Loss: 0.459236  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5709\n",
      "Test loss: 0.579992 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Loss: 0.754595  [    0/ 8000 samples]\n",
      "Loss: 0.586460  [ 3200/ 8000 samples]\n",
      "Loss: 0.655672  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5777\n",
      "Test loss: 0.540221 \n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "Loss: 0.748986  [    0/ 8000 samples]\n",
      "Loss: 0.628740  [ 3200/ 8000 samples]\n",
      "Loss: 0.745529  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5582\n",
      "Test loss: 0.550187 \n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "Loss: 0.632046  [    0/ 8000 samples]\n",
      "Loss: 0.821629  [ 3200/ 8000 samples]\n",
      "Loss: 0.472405  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5798\n",
      "Test loss: 0.520611 \n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "Loss: 0.637634  [    0/ 8000 samples]\n",
      "Loss: 0.398172  [ 3200/ 8000 samples]\n",
      "Loss: 0.549233  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5435\n",
      "Test loss: 0.561906 \n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "Loss: 0.629536  [    0/ 8000 samples]\n",
      "Loss: 0.487574  [ 3200/ 8000 samples]\n",
      "Loss: 0.508301  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5526\n",
      "Test loss: 0.590893 \n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "Loss: 0.756281  [    0/ 8000 samples]\n",
      "Loss: 0.332477  [ 3200/ 8000 samples]\n",
      "Loss: 0.567078  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5702\n",
      "Test loss: 0.557041 \n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "Loss: 0.534921  [    0/ 8000 samples]\n",
      "Loss: 0.685921  [ 3200/ 8000 samples]\n",
      "Loss: 0.636906  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5550\n",
      "Test loss: 0.491260 \n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "Loss: 0.480299  [    0/ 8000 samples]\n",
      "Loss: 0.646641  [ 3200/ 8000 samples]\n",
      "Loss: 0.619911  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5649\n",
      "Test loss: 0.555965 \n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "Loss: 0.585118  [    0/ 8000 samples]\n",
      "Loss: 0.743267  [ 3200/ 8000 samples]\n",
      "Loss: 0.560319  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5755\n",
      "Test loss: 0.583743 \n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "Loss: 0.422756  [    0/ 8000 samples]\n",
      "Loss: 0.655347  [ 3200/ 8000 samples]\n",
      "Loss: 0.538616  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5609\n",
      "Test loss: 0.535356 \n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "Loss: 0.711176  [    0/ 8000 samples]\n",
      "Loss: 0.393643  [ 3200/ 8000 samples]\n",
      "Loss: 0.528513  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5560\n",
      "Test loss: 0.557448 \n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "Loss: 0.378375  [    0/ 8000 samples]\n",
      "Loss: 0.492207  [ 3200/ 8000 samples]\n",
      "Loss: 0.565697  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5463\n",
      "Test loss: 0.536844 \n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "Loss: 0.524660  [    0/ 8000 samples]\n",
      "Loss: 0.475952  [ 3200/ 8000 samples]\n",
      "Loss: 0.502643  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5561\n",
      "Test loss: 0.595055 \n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "Loss: 0.512075  [    0/ 8000 samples]\n",
      "Loss: 0.602323  [ 3200/ 8000 samples]\n",
      "Loss: 0.431868  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5443\n",
      "Test loss: 0.577603 \n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "Loss: 0.501120  [    0/ 8000 samples]\n",
      "Loss: 0.561615  [ 3200/ 8000 samples]\n",
      "Loss: 0.525155  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5814\n",
      "Test loss: 0.566387 \n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "Loss: 0.536493  [    0/ 8000 samples]\n",
      "Loss: 0.655895  [ 3200/ 8000 samples]\n",
      "Loss: 0.512987  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5416\n",
      "Test loss: 0.552227 \n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "Loss: 0.752701  [    0/ 8000 samples]\n",
      "Loss: 0.572658  [ 3200/ 8000 samples]\n",
      "Loss: 0.433083  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5481\n",
      "Test loss: 0.528291 \n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "Loss: 0.565629  [    0/ 8000 samples]\n",
      "Loss: 0.587666  [ 3200/ 8000 samples]\n",
      "Loss: 0.545912  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5591\n",
      "Test loss: 0.562789 \n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "Loss: 0.453973  [    0/ 8000 samples]\n",
      "Loss: 0.408656  [ 3200/ 8000 samples]\n",
      "Loss: 0.421344  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5436\n",
      "Test loss: 0.581672 \n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "Loss: 0.669739  [    0/ 8000 samples]\n",
      "Loss: 0.589778  [ 3200/ 8000 samples]\n",
      "Loss: 0.470211  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5395\n",
      "Test loss: 0.550324 \n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "Loss: 0.425339  [    0/ 8000 samples]\n",
      "Loss: 0.446186  [ 3200/ 8000 samples]\n",
      "Loss: 0.967224  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5649\n",
      "Test loss: 0.569596 \n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "Loss: 0.798042  [    0/ 8000 samples]\n",
      "Loss: 0.465676  [ 3200/ 8000 samples]\n",
      "Loss: 0.592351  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5355\n",
      "Test loss: 0.543838 \n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "Loss: 0.730545  [    0/ 8000 samples]\n",
      "Loss: 0.543775  [ 3200/ 8000 samples]\n",
      "Loss: 0.661809  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5427\n",
      "Test loss: 0.532148 \n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "Loss: 0.619865  [    0/ 8000 samples]\n",
      "Loss: 0.699751  [ 3200/ 8000 samples]\n",
      "Loss: 0.596161  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5486\n",
      "Test loss: 0.544186 \n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "Loss: 0.493579  [    0/ 8000 samples]\n",
      "Loss: 0.580587  [ 3200/ 8000 samples]\n",
      "Loss: 0.540189  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5472\n",
      "Test loss: 0.560207 \n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "Loss: 0.581680  [    0/ 8000 samples]\n",
      "Loss: 0.798365  [ 3200/ 8000 samples]\n",
      "Loss: 0.562757  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5300\n",
      "Test loss: 0.527002 \n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "Loss: 0.596274  [    0/ 8000 samples]\n",
      "Loss: 0.466642  [ 3200/ 8000 samples]\n",
      "Loss: 0.486219  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5331\n",
      "Test loss: 0.539981 \n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "Loss: 0.599124  [    0/ 8000 samples]\n",
      "Loss: 0.427744  [ 3200/ 8000 samples]\n",
      "Loss: 0.570230  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5367\n",
      "Test loss: 0.540381 \n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "Loss: 0.581705  [    0/ 8000 samples]\n",
      "Loss: 0.599106  [ 3200/ 8000 samples]\n",
      "Loss: 0.755022  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5650\n",
      "Test loss: 0.582458 \n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "Loss: 0.773239  [    0/ 8000 samples]\n",
      "Loss: 0.548717  [ 3200/ 8000 samples]\n",
      "Loss: 0.297432  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5424\n",
      "Test loss: 0.623048 \n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "Loss: 0.624030  [    0/ 8000 samples]\n",
      "Loss: 0.502195  [ 3200/ 8000 samples]\n",
      "Loss: 0.456030  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5434\n",
      "Test loss: 0.583087 \n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "Loss: 0.455788  [    0/ 8000 samples]\n",
      "Loss: 0.487983  [ 3200/ 8000 samples]\n",
      "Loss: 0.837687  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5563\n",
      "Test loss: 0.573181 \n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "Loss: 0.793306  [    0/ 8000 samples]\n",
      "Loss: 0.478445  [ 3200/ 8000 samples]\n",
      "Loss: 0.507757  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5449\n",
      "Test loss: 0.558546 \n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "Loss: 0.394007  [    0/ 8000 samples]\n",
      "Loss: 0.457837  [ 3200/ 8000 samples]\n",
      "Loss: 0.588305  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5262\n",
      "Test loss: 0.534465 \n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "Loss: 0.379923  [    0/ 8000 samples]\n",
      "Loss: 0.434368  [ 3200/ 8000 samples]\n",
      "Loss: 0.491141  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5533\n",
      "Test loss: 0.543945 \n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "Loss: 0.600936  [    0/ 8000 samples]\n",
      "Loss: 0.691748  [ 3200/ 8000 samples]\n",
      "Loss: 0.545448  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5393\n",
      "Test loss: 0.543485 \n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "Loss: 0.468364  [    0/ 8000 samples]\n",
      "Loss: 0.533135  [ 3200/ 8000 samples]\n",
      "Loss: 0.819053  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5749\n",
      "Test loss: 0.522342 \n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "Loss: 0.427750  [    0/ 8000 samples]\n",
      "Loss: 0.527102  [ 3200/ 8000 samples]\n",
      "Loss: 0.513645  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5493\n",
      "Test loss: 0.552052 \n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "Loss: 0.590854  [    0/ 8000 samples]\n",
      "Loss: 0.498456  [ 3200/ 8000 samples]\n",
      "Loss: 0.490465  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5426\n",
      "Test loss: 0.572200 \n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "Loss: 0.367068  [    0/ 8000 samples]\n",
      "Loss: 0.597455  [ 3200/ 8000 samples]\n",
      "Loss: 0.523920  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5709\n",
      "Test loss: 0.577957 \n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "Loss: 0.539253  [    0/ 8000 samples]\n",
      "Loss: 0.657435  [ 3200/ 8000 samples]\n",
      "Loss: 0.736858  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5494\n",
      "Test loss: 0.517613 \n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "Loss: 0.644602  [    0/ 8000 samples]\n",
      "Loss: 0.476313  [ 3200/ 8000 samples]\n",
      "Loss: 0.547726  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5493\n",
      "Test loss: 0.578740 \n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "Loss: 0.508374  [    0/ 8000 samples]\n",
      "Loss: 0.679747  [ 3200/ 8000 samples]\n",
      "Loss: 0.572461  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5382\n",
      "Test loss: 0.562771 \n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "Loss: 0.451676  [    0/ 8000 samples]\n",
      "Loss: 0.575805  [ 3200/ 8000 samples]\n",
      "Loss: 0.853194  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5383\n",
      "Test loss: 0.498859 \n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "Loss: 0.641619  [    0/ 8000 samples]\n",
      "Loss: 0.616476  [ 3200/ 8000 samples]\n",
      "Loss: 0.532338  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5457\n",
      "Test loss: 0.578039 \n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "Loss: 0.532288  [    0/ 8000 samples]\n",
      "Loss: 0.478613  [ 3200/ 8000 samples]\n",
      "Loss: 0.457092  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5368\n",
      "Test loss: 0.570814 \n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "Loss: 0.509193  [    0/ 8000 samples]\n",
      "Loss: 0.542077  [ 3200/ 8000 samples]\n",
      "Loss: 0.335894  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5436\n",
      "Test loss: 0.557439 \n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "Loss: 0.632780  [    0/ 8000 samples]\n",
      "Loss: 0.816798  [ 3200/ 8000 samples]\n",
      "Loss: 0.777244  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5447\n",
      "Test loss: 0.512238 \n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "Loss: 0.522286  [    0/ 8000 samples]\n",
      "Loss: 0.475118  [ 3200/ 8000 samples]\n",
      "Loss: 0.546137  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5250\n",
      "Test loss: 0.497246 \n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "Loss: 0.517862  [    0/ 8000 samples]\n",
      "Loss: 0.522554  [ 3200/ 8000 samples]\n",
      "Loss: 0.481077  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5379\n",
      "Test loss: 0.576770 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Loss: 0.352716  [    0/ 8000 samples]\n",
      "Loss: 0.588317  [ 3200/ 8000 samples]\n",
      "Loss: 0.525960  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5531\n",
      "Test loss: 0.500981 \n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "Loss: 0.522243  [    0/ 8000 samples]\n",
      "Loss: 0.490051  [ 3200/ 8000 samples]\n",
      "Loss: 0.434902  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5456\n",
      "Test loss: 0.538944 \n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "Loss: 0.456302  [    0/ 8000 samples]\n",
      "Loss: 0.647687  [ 3200/ 8000 samples]\n",
      "Loss: 0.290730  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5284\n",
      "Test loss: 0.506782 \n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "Loss: 0.445603  [    0/ 8000 samples]\n",
      "Loss: 0.494369  [ 3200/ 8000 samples]\n",
      "Loss: 0.365899  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5289\n",
      "Test loss: 0.545217 \n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "Loss: 0.456960  [    0/ 8000 samples]\n",
      "Loss: 0.474747  [ 3200/ 8000 samples]\n",
      "Loss: 0.685887  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5324\n",
      "Test loss: 0.526311 \n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "Loss: 0.572279  [    0/ 8000 samples]\n",
      "Loss: 0.535816  [ 3200/ 8000 samples]\n",
      "Loss: 0.557238  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5313\n",
      "Test loss: 0.549658 \n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "Loss: 0.563806  [    0/ 8000 samples]\n",
      "Loss: 0.448704  [ 3200/ 8000 samples]\n",
      "Loss: 0.511910  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5509\n",
      "Test loss: 0.576668 \n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "Loss: 0.502667  [    0/ 8000 samples]\n",
      "Loss: 0.698736  [ 3200/ 8000 samples]\n",
      "Loss: 0.502121  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5295\n",
      "Test loss: 0.533293 \n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "Loss: 0.420611  [    0/ 8000 samples]\n",
      "Loss: 0.562149  [ 3200/ 8000 samples]\n",
      "Loss: 0.571627  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5515\n",
      "Test loss: 0.558212 \n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "Loss: 0.512383  [    0/ 8000 samples]\n",
      "Loss: 0.643745  [ 3200/ 8000 samples]\n",
      "Loss: 0.772632  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5420\n",
      "Test loss: 0.528956 \n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "Loss: 0.535294  [    0/ 8000 samples]\n",
      "Loss: 0.642247  [ 3200/ 8000 samples]\n",
      "Loss: 0.607220  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5599\n",
      "Test loss: 0.532750 \n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "Loss: 0.460606  [    0/ 8000 samples]\n",
      "Loss: 0.660404  [ 3200/ 8000 samples]\n",
      "Loss: 0.434813  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5556\n",
      "Test loss: 0.571417 \n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "Loss: 0.450238  [    0/ 8000 samples]\n",
      "Loss: 0.601021  [ 3200/ 8000 samples]\n",
      "Loss: 0.891434  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5473\n",
      "Test loss: 0.522978 \n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "Loss: 0.425883  [    0/ 8000 samples]\n",
      "Loss: 0.475224  [ 3200/ 8000 samples]\n",
      "Loss: 0.559409  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5273\n",
      "Test loss: 0.549030 \n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "Loss: 0.563969  [    0/ 8000 samples]\n",
      "Loss: 0.552755  [ 3200/ 8000 samples]\n",
      "Loss: 0.411403  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5649\n",
      "Test loss: 0.533918 \n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "Loss: 0.583006  [    0/ 8000 samples]\n",
      "Loss: 0.617442  [ 3200/ 8000 samples]\n",
      "Loss: 0.882718  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5170\n",
      "Test loss: 0.506460 \n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "Loss: 0.564771  [    0/ 8000 samples]\n",
      "Loss: 0.548856  [ 3200/ 8000 samples]\n",
      "Loss: 0.530428  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5221\n",
      "Test loss: 0.495313 \n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "Loss: 0.516259  [    0/ 8000 samples]\n",
      "Loss: 0.591409  [ 3200/ 8000 samples]\n",
      "Loss: 0.591663  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5336\n",
      "Test loss: 0.621637 \n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "Loss: 0.639626  [    0/ 8000 samples]\n",
      "Loss: 0.578430  [ 3200/ 8000 samples]\n",
      "Loss: 0.444080  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5435\n",
      "Test loss: 0.528615 \n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "Loss: 0.386268  [    0/ 8000 samples]\n",
      "Loss: 0.335080  [ 3200/ 8000 samples]\n",
      "Loss: 0.431714  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5270\n",
      "Test loss: 0.556667 \n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "Loss: 0.521793  [    0/ 8000 samples]\n",
      "Loss: 0.503950  [ 3200/ 8000 samples]\n",
      "Loss: 0.532384  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5493\n",
      "Test loss: 0.554033 \n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "Loss: 0.303435  [    0/ 8000 samples]\n",
      "Loss: 0.518483  [ 3200/ 8000 samples]\n",
      "Loss: 0.514240  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5458\n",
      "Test loss: 0.510262 \n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "Loss: 0.626966  [    0/ 8000 samples]\n",
      "Loss: 0.978959  [ 3200/ 8000 samples]\n",
      "Loss: 0.579892  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5589\n",
      "Test loss: 0.566577 \n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "Loss: 0.461902  [    0/ 8000 samples]\n",
      "Loss: 0.419929  [ 3200/ 8000 samples]\n",
      "Loss: 0.721311  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5379\n",
      "Test loss: 0.493328 \n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "Loss: 0.556161  [    0/ 8000 samples]\n",
      "Loss: 0.562957  [ 3200/ 8000 samples]\n",
      "Loss: 0.574753  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5496\n",
      "Test loss: 0.538918 \n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "Loss: 0.632383  [    0/ 8000 samples]\n",
      "Loss: 0.574903  [ 3200/ 8000 samples]\n",
      "Loss: 0.510384  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5373\n",
      "Test loss: 0.493112 \n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "Loss: 0.487152  [    0/ 8000 samples]\n",
      "Loss: 0.669472  [ 3200/ 8000 samples]\n",
      "Loss: 0.539585  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5332\n",
      "Test loss: 0.538150 \n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "Loss: 0.618863  [    0/ 8000 samples]\n",
      "Loss: 0.549534  [ 3200/ 8000 samples]\n",
      "Loss: 0.530558  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5471\n",
      "Test loss: 0.504474 \n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "Loss: 0.457524  [    0/ 8000 samples]\n",
      "Loss: 0.407652  [ 3200/ 8000 samples]\n",
      "Loss: 0.476632  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5209\n",
      "Test loss: 0.547720 \n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "Loss: 0.550479  [    0/ 8000 samples]\n",
      "Loss: 0.596347  [ 3200/ 8000 samples]\n",
      "Loss: 0.427218  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5210\n",
      "Test loss: 0.508712 \n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "Loss: 0.482319  [    0/ 8000 samples]\n",
      "Loss: 0.519061  [ 3200/ 8000 samples]\n",
      "Loss: 0.704068  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5572\n",
      "Test loss: 0.576556 \n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "Loss: 0.393921  [    0/ 8000 samples]\n",
      "Loss: 0.495033  [ 3200/ 8000 samples]\n",
      "Loss: 0.383799  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5323\n",
      "Test loss: 0.540888 \n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "Loss: 0.664147  [    0/ 8000 samples]\n",
      "Loss: 0.746435  [ 3200/ 8000 samples]\n",
      "Loss: 0.969734  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5326\n",
      "Test loss: 0.526378 \n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "Loss: 0.601841  [    0/ 8000 samples]\n",
      "Loss: 0.710000  [ 3200/ 8000 samples]\n",
      "Loss: 0.511431  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5359\n",
      "Test loss: 0.482978 \n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "Loss: 0.618631  [    0/ 8000 samples]\n",
      "Loss: 0.468849  [ 3200/ 8000 samples]\n",
      "Loss: 0.505464  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5209\n",
      "Test loss: 0.551198 \n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "Loss: 0.567348  [    0/ 8000 samples]\n",
      "Loss: 0.515479  [ 3200/ 8000 samples]\n",
      "Loss: 0.375692  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5341\n",
      "Test loss: 0.526610 \n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "Loss: 0.550373  [    0/ 8000 samples]\n",
      "Loss: 0.549604  [ 3200/ 8000 samples]\n",
      "Loss: 0.612426  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5431\n",
      "Test loss: 0.554740 \n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "Loss: 0.642068  [    0/ 8000 samples]\n",
      "Loss: 0.462761  [ 3200/ 8000 samples]\n",
      "Loss: 0.559237  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5307\n",
      "Test loss: 0.556389 \n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "Loss: 0.744892  [    0/ 8000 samples]\n",
      "Loss: 0.445721  [ 3200/ 8000 samples]\n",
      "Loss: 0.517374  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5378\n",
      "Test loss: 0.572765 \n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "Loss: 0.324759  [    0/ 8000 samples]\n",
      "Loss: 0.358647  [ 3200/ 8000 samples]\n",
      "Loss: 0.498409  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5232\n",
      "Test loss: 0.542506 \n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "Loss: 0.443489  [    0/ 8000 samples]\n",
      "Loss: 0.620737  [ 3200/ 8000 samples]\n",
      "Loss: 0.594603  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5405\n",
      "Test loss: 0.546445 \n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "Loss: 0.634300  [    0/ 8000 samples]\n",
      "Loss: 0.663972  [ 3200/ 8000 samples]\n",
      "Loss: 0.691160  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5386\n",
      "Test loss: 0.523114 \n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "Loss: 0.541009  [    0/ 8000 samples]\n",
      "Loss: 0.369489  [ 3200/ 8000 samples]\n",
      "Loss: 0.448889  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5218\n",
      "Test loss: 0.552311 \n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "Loss: 0.535967  [    0/ 8000 samples]\n",
      "Loss: 0.713525  [ 3200/ 8000 samples]\n",
      "Loss: 0.567938  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5634\n",
      "Test loss: 0.554346 \n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "Loss: 0.712171  [    0/ 8000 samples]\n",
      "Loss: 0.478839  [ 3200/ 8000 samples]\n",
      "Loss: 0.742553  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5476\n",
      "Test loss: 0.548128 \n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "Loss: 0.578461  [    0/ 8000 samples]\n",
      "Loss: 0.684592  [ 3200/ 8000 samples]\n",
      "Loss: 0.699140  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5424\n",
      "Test loss: 0.576479 \n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "Loss: 0.553732  [    0/ 8000 samples]\n",
      "Loss: 0.664573  [ 3200/ 8000 samples]\n",
      "Loss: 0.607767  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5278\n",
      "Test loss: 0.555266 \n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "Loss: 0.903529  [    0/ 8000 samples]\n",
      "Loss: 0.644345  [ 3200/ 8000 samples]\n",
      "Loss: 0.556145  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5366\n",
      "Test loss: 0.532459 \n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "Loss: 0.376399  [    0/ 8000 samples]\n",
      "Loss: 0.727389  [ 3200/ 8000 samples]\n",
      "Loss: 0.408419  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5474\n",
      "Test loss: 0.544250 \n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "Loss: 0.416436  [    0/ 8000 samples]\n",
      "Loss: 0.399451  [ 3200/ 8000 samples]\n",
      "Loss: 0.612650  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5176\n",
      "Test loss: 0.530720 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Loss: 0.523350  [    0/ 8000 samples]\n",
      "Loss: 0.735975  [ 3200/ 8000 samples]\n",
      "Loss: 0.347284  [ 6400/ 8000 samples]\n",
      "Average loss: 0.5221\n",
      "Test loss: 0.500620 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "epochs = 500\n",
    "train_history = []\n",
    "test_history = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    loss = train_loop(train_dataloader, model, optimizer)\n",
    "    train_history.append(loss)\n",
    "    loss = test_loop(test_dataloader, model)\n",
    "    test_history.append(loss)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1727147910922,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "2AI1HOv34t6F",
    "outputId": "9dee6672-d8f5-4d10-82ae-5061542bfb72"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(1, len(train_history) + 1)\n",
    "plt.plot(epochs, pd.Series(train_history).rolling(10).mean(),\n",
    "         '-o', label = 'train loss')\n",
    "plt.plot(epochs, pd.Series(train_history).rolling(10).mean(),\n",
    "         '-o', label = 'test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Conditional Flow Matching')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dRs_EbK4t6J"
   },
   "source": [
    "### 4. Visualize posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 40049,
     "status": "ok",
     "timestamp": 1727148096452,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "ULdatNgc4t6J",
    "outputId": "6f597ec7-1e81-43c5-a0d5-1ced25c91929"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m num_posteriors \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      2\u001b[0m num_eval_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1_000\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_posteriors):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "num_posteriors = 3\n",
    "num_eval_samples = 1_000\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for n in range(num_posteriors):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        test_x, test_y = test_dataset[n]\n",
    "       \n",
    "        # Sample the posterior\n",
    "        test_y = test_y.expand(num_eval_samples, *test_y.shape)\n",
    "        pred_samples =  ... # TO Sample \n",
    "    \n",
    "        # Undo the standardization\n",
    "        parameters_mean = waveform_dataset.parameters_mean.numpy()\n",
    "        parameters_std  = waveform_dataset.parameters_std.numpy()\n",
    "        \n",
    "        pred_samples = parameters_mean + pred_samples   * parameters_std\n",
    "        truth        = parameters_mean + test_x.numpy() * parameters_std\n",
    "    \n",
    "        # Plot\n",
    "        corner.corner(pred_samples, truths=truth, labels=['$m_1$', '$m_2$'],\n",
    "                      color='mediumaquamarine',truth_color='k')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/odsl-team/genAI-Days/blob/main/02_flow_matching/02_flow_matching.ipynb",
     "timestamp": 1727149080412
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
